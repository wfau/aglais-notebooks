{
  "metadata": {
    "name": "1. Start here",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003c!--\n\n    Gaia Data Processing and Analysis Consortium (DPAC) \n    Co-ordination Unit 9 Work Package 930\n    \n    (c) 2005-2025 Gaia DPAC\n    \n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see \u003chttps://www.gnu.org/licenses/\u003e.\n    --\u003e\n\nThis Gaia science exploitation platform is built on [Apache Spark](https://spark.apache.org). The primary user interface uses web-based [Zeppelin notebooks](https://zeppelin.apache.org/docs/0.10.0/quickstart/explore_ui.html#note-layout) (similar to the possibly more familiar Jupyter notebook interface) employing Python code and a suite of third-party packages and libraries for analysis, plotting, etc. A working knowledge of Gaia data releases to-date, Python and Structured Query Language is assumed.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The best way to familiarise yourself with the platform is to examine the example notebooks that come bundled with it. These are:\n\n1. Start here: platform familiarisation (this notebook);\n1. Data holdings: a description of the data hosted on the platform;\n1. Source counts over the sky: a simple example of querying and plotting;\n1. Mean proper motions over the sky: another simple example;\n1. Working with Gaia XP spectra: demonstrates trawling the large XP spectral dataset, and plotting of the spectra;\n1. Analyses involving cross-match surveys: using Gaia data in conjunction with optical and infrared survey data;\n1. Good astrometric solutions via ML Random Forest classifier: a more complex workflow demonstrating use of the Spark ML library in supervised machine learning;\n1. Tips and tricks: some further usage tips and technical aspects of the platform including interactions and plotting.\n\nTo load a notebook, click on the \"Notebook\" tab (above left) and select. Copies of the public examples can be found under each user\u0027s private folder under the top level \"Users\". Note that if you load these notebooks from the top level read-only folder \"Public examples\" you will not be able to edit or execute cells, whereas if you load from your own private copies then you can do anything you like, including editing and running.\n\nNewcomers are advised to work through at least the first four notebooks before attempting to use the system. Also have a quick look at the Zeppelin link in the previous cell first to get oriented in the User Interface. "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This and all the other example notebooks itemised above are [Zeppelin notebooks](https://zeppelin.apache.org/docs/0.10.0/quickstart/explore_ui.html#note-layout). Text, code (in different languages) and plots can be freely mixed in the notebook in paragraphs, or cells. For example, to see the [mark-down code](https://sourceforge.net/p/zeppelin/wiki/markdown_syntax/) for this cell simply click on the outward-pointing arrows icon in the top right of the cell; click again to hide the code leaving only the output. Output is created by executing the cell: press the \"play\" icon in the top right. Other functionality is provided under the cog icon while buttons at the top of the web interface provide further management functions. To save a notebook locally, click on one of the \"Export this note\" icons at the top: doing so will create a local file with extension \".zpln\" or \".ipynb\" depending on which is chosen (both areJavaScript Object Notation format, which is not amenable to viewing/editing outside of the UI). To import a previously saved note (or indeed one provisioned externally) click on the Zeppelin icon in the top left of the UI and then click \"Import note\". Files exported as \".ipynb\" conveniently can be uploaded to Jupyter Notebook for preview elsewhere.  \n\nPlease note that:\n* You are strongly advised to save locally all your notebook edits at frequent intervals (via one of the \"Export this note\" buttons). Preservation of the notebook content of the platform can be managed on a best-efforts basis only. While we will endeavour to restore fully in the event of loss of the deployed system or Cloud services on which it is built, we can make no guarantee that your work will be backed up and will be recovered; \n* By default your notebooks are private to your account on the platform. If you wish to share with other users you can do this by altering the \"Note permissions\" - press the padlock button in the top right of your notebook to do this;\n* Important etiquette: the Gaia DMP is a resource shared between users. Please be considerate to fellow users by logging out of the web interface when you are not using it. In particular please do not leave notebooks open and holding cached data when you are not using them - further examples of best practice are given below.  \n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The platform environment employs [Spark (version 3.0)](https://spark.apache.org/docs/3.0.0/) for distributed processing and scalability to high-volume data releases. For convenience a high-level application programming interface (API) is available through Python to access and analyse large data sets. The primary distributed data container is known as a [\"data frame\"](https://spark.apache.org/docs/3.0.0/sql-programming-guide.html), a richly structured object encapsulating data, descriptions and methods that is _not_ limited to simple tables with primitive column types in contrast to systems built on relational data base technology (RDBMS). For ease of use the [API features Structured Query Language](https://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html) as a familiar way to create and manipulate data objects as the initial steps in work flows. Note that this is not Astronomy Data Query Language (ADQL): astronomy-specific aspects, in particular geometric functions for spatial queries, are not available in this flavour of SQL. However PySpark SQL offers a much greater degree of end-user programmability than ADQL interfaces built in front of RDBMS, for example [User Defined Functions](https://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.UDFRegistration) that can be integrated into SQL statements. The fact that SQL features prominently in the API should not be taken to mean that this platform aims to replicate the functionality of existing relational systems. Obviously, usage scenarios best served by such systems should be targetted there. This platform is designed to facilitate scale-out usage scenarios, e.g. large-scale statistical analyses, that cannot be achieved through relational systems built on TAP/ADQL. (In fact on this platform you can mix-and-match both by calling out to any IVOA TAP/ADQL service as part of a larger work flow.)\n\nOn start-up and by default, notebooks launch in a Spark context with the data resources already set up but we recommend importing the catalogue set up at the head of each notebook as follows (for example in case the context is reset and the notebook is re-executed from the top), e.g.\n\n    %pyspark\n    \n    # standard Gaia data mining platform set-up\n    import gaiadmpsetup\n    \n    # echo details of the available data resources\n    for line in spark.catalog.listTables(): print (line)\n\n... see the next code cell. The notebook interpreter can be set for individual cells as is done here and in the next: \"%md\" for mark-down, \"%pyspark\" for the PySpark python code etc.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# standard Gaia data mining platform set-up\nimport gaiadmpsetup\n\n# echo details of the available data resources\nfor line in spark.catalog.listTables(): print (line)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The operating model of Spark as actioned via a workflow expressed in a notebook will be unfamiliar to many. Users should be aware of the following basic facts from the outset:\n\n__Lazy Evaluation:__ Spark employs \"lazy evaluation\" of jobs: nothing happens until the last possible point in the workflow. For example, a notebook cell may contain a data frame defined by a query \n\n    df \u003d spark.sql(\"select * from gaia_source\")\n\nbut as far as Spark is concerned this is simply a definiton, or \"transformation\", of a data resource. If nothing is done with the data frame defined by that query in the cell, then the cell will appear to run instantaneously. Only if an \"action\" is made on the data frame subsequently will physical execution take place. This means, for example, that if a cell contains a run-time error this may not show up until a later cell is executed with the exception trace explaining the error appearing in that later cell\u0027s output (as opposed to any output associated with the cell containing the source of the error). Note also that the statement above, on it\u0027s own, will not result in the entire billon+ row, ~100 column, tera-byte scale data set being loaded into memory. Whatever is expressed in subsequent transformations (e.g. SQL \"where\" clauses) and actions (e.g. aggregations) will define \"filter push-down\" optimisations that result in only those data needed to action the process being read into memory and processed.  \n\n__Caching:__ Actions expressed in a cell may need to be re-executed if needed as part of the execution plan for actions in a subsequent cell. This may result in expensive operations being repeatedly executed when developing notebook workflows and working on cells in isolation. You can tell Spark to cache a data frame explicitly to avoid this and speed-up significantly subsequent cell operations\n\n    df \u003d spark.sql(\"select column_1, column_2, ... from gaia_source where ...\").cache()\n    \nbut obviously this should only be done within reason and with due regard to the amount of local memory/disk required by the worker nodes: make sure you only cache what you really need and _in particular always free up cached resources at the end of your notebook session:_\n\n\n    sqlContext.clearCache()\n\nFor further details see [this useful article on best practice with Spark caching](https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34). Some of the example notebooks provided use caching to expedite the execution of cells that re-use data frames.\n\n__Indexing:__ The data hosted on this platform are arranged by gaia_source.source_id as the primary unique identifier in Gaia Data Releases. This key should be employed when joining distinct data sets, e.g. pre-computed best neighbours from external survey catalogues - see the \"Data holdings\" notebook. It can also be used to pull out specific records quickly:\n\n    spark.sql(\"select * from gaia_source where source_id \u003d ...\").show()\n   \nOtherwise, the general RDBMS paradigm employing SQL indexes to optimise execution plans of queries predicated on certain columns has no equivalent here. It is worth noting however that the combination of \"filter push-down\" and the underlying storage format ([Parquet](https://parquet.apache.org/docs/)) result in only those data that are needed being read from disk on this platform. This is in contrast to RDBMS where columns are generally read from pages within disk files regardless of need unless covered by an index employed as part of an optimised execution plan. This is one of the ways in which Spark achieves much faster performance in \"big data\" analysis.\n\n__Distributed processing model:__ Fast performance in handling very large data sets is achieved via cluster parallelism with actions split up and optimised for execution on concurrent workers. This is achievable only if the user\u0027s processing works with the Spark data frame which in turn encapsulates a Spark Resilient Distributed Dataset (RDD). Note that issuing a `sparkDataFrame.collect()`, or `sparkDataFrame.toPandas()` for example, retrieves the distributed components of the Spark data frame to a single driver process and subsequent actions will be no longer distributed over the cluster of workers. Care should be taken in the final stages of work flows to avoid Out Of Memory exceptions by collecting too large a results set to the driver process.\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In addition to the example notebooks provided here and the linked resources therein see the following for further information:\n\n* [PySpark documentation](https://spark.apache.org/docs/latest/api/python/index.html)\n* [PySpark SQL cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf)\n\n"
    }
  ]
}