{
  "metadata": {
    "name": "8. Tips and tricks",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003c!--\n\n    Gaia Data Processing and Analysis Consortium (DPAC) \n    Co-ordination Unit 9 Work Package 930, based on \n    original scripts provided by the Apache SW Foundation\n    \n    (c) 2005-2025 Gaia DPAC\n    \n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see \u003chttps://www.gnu.org/licenses/\u003e.\n    --\u003e\n    \nThis notebook illustrates a few tips and tricks that should aid users in interacting with the platform via the Zeppelin user interface."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Users should be aware that the platform accesses a shared compute cluster. Depending on activity elsewhere cell execution may result in \"Pending\" jobs and we kindly request that users be patient. \n\nIf, however, a running notebook cell becomes unresponsive (e.g. goes away running and never comes back) or behaves in unexpected ways (the Python interpreter can get occasionally tied in knots) as a last resort you can reset using the \"Interpreter binding\" drop-down available from the top-level cog icon at the head of the notebook (N.B. _not_ the individual cog icon to the immediate upper-right of this cell). Click on this top-level cog then click on the circular arrows icon in the drop-down: this is the \"Restart\" button. Note that this will kill all currently executing jobs in your context and free up all memory so you must re-establish the platform set-up in the resulting fresh Spark context by executing in PySpark\n\n    import gaiadmpsetup\n\nbefore doing anything else. (This is why we recommend that you include this line at the top of each notebook workflow: if the platform is already set up then this import does nothing so there\u0027s no harm in including it and no penalty in importing multiple times if/when re-running the notebook from the top)."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You can export data from the platform via your Zeppelin account home directory, onto your local desktop. In order to do this you need to install your public ssh key on the system. The best way to do this is send us a copy of your public ssh key in a email and we will add it to the system in the correct location. If you are unsure how to find your public ssh key send us an email and we will talk you through it.\n\nTo transfer a file out of the system, copy the data file you wish to export into your /home or /user directory on the system,\nFor example:\n\n    %sh\n    echo \"my data\" \u003e /user/{YOUR-USERNAME}/data.txt\n\nFrom your local machine you can now either ssh into Zeppelin e.g. from your own desktop / laptop\n\n    ssh {YOUR-USERNAME}@dmp.gaia.ac.uk\n    \nor you can copy your files from Zeppelin using scp e.g. from your local desktop (replace ‘data.txt’ with the name of the file you want to download, and /tmp/data with the path on your local desktop where you want to store it)\n\n    scp {YOUR-USERNAME}@dmp.gaia.ac.uk:/user/{YOUR-USERNAME}/data.txt /tmp/data\n    \nTo save in-memory data (for example as expressed in a results DataFrame) on the platform in a file for export, be aware that a Spark DataFrame is a distributed data set. If you save such an object to disk you will get a large set of partition files reflecting the natural distribution of the underlying source data. This is neither convenient nor particularly friendly. Provided the data size is not too large it is better to collect the distributed data to a non-distributed object on the master executor. The easiest way to do this is to call the \"toPandas\" method of the DataFrame, then this can be saved to a convenient format (e.g. comma-separated value). In the following simple example a DataFrame of the positions and magnitudes of all sources in the [Gaia Andromeda Photometric Survey (GAPS)](https://gea.esac.esa.int/archive/documentation/GDR3/Data_processing/chap_cu5pho/sec_cu5pho_gaps/) is created, collected to an intermediate non-distributed Pandas object, then saved to csv: "
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# simple example of saving a results file to disk, e.g. prior to transfer off the platform to a user\u0027s local file system (see final paragraph in the description in the cell above)\n\n# standard set-up\nimport gaiadmpsetup\n\n# create an example data set - in this case a simple GAPS selection from gaia source\ndata_frame \u003d spark.sql(\u0027SELECT ra, dec, parallax, parallax_error, phot_g_mean_mag, phot_bp_mean_mag, phot_rp_mean_mag FROM gaiadr3.gaia_source WHERE in_andromeda_survey\u0027)\n\n# collect results to Pandas and save to csv in the user\u0027s home directory - substitute your username as appropriate\n# data_frame.toPandas().to_csv(path_or_buf \u003d \u0027/user/{YOUR-USERNAME}/gapscat.csv\u0027, index \u003d False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Interpreters\n\nWe recommend usage of the PySpark interpreter since this gives access to large scale distributed computing via the data frame applications programming interface. Other Python interpreters are available however: for light, non-distributed processing of relatively small data sets collected to the driver process in Zeppelin it is possible to specify plain Python or IPython interpreters (the latter is perhaps more familiar to Jupyter notebook users). \n\nThere are some differences in functionality between the interpreters available - these are illustrated in the following cells."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport sys\n\nhelp(sys)\n# ... available also in python.ipython interpreter"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n# ... this facility is not available in the pyspark interpreter\n\nimport sys\n\nsys?"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "All the IPython magic functions are avalible in Zeppelin, here\u0027s one example of `%timeit`, for the complete IPython magic functions, you can check the [link](http://ipython.readthedocs.io/en/stable/interactive/magics.html) here.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n# ... available only in IPython interpreter\n\n%timeit range(1000)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Tab completion\n\nTab completion, especially for attributes, is a convenient way to explore the structure of any object you’re dealing with. Simply type `object_name.\u003cTAB\u003e` to view the object’s attributes. See the following screenshot illustrating how tab completion works in the IPython Interpreter; it will work also in the pyspark interpreter.\n![alt text](https://user-images.githubusercontent.com/164491/34858941-3f28105a-f78e-11e7-8341-2fbfd306ba5b.gif \"Logo Title Text 1\")\n\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Use of the ZeppelinContext \n\n`ZeppelinContext` is a utlity class which provide the following features\n\n* Dynamic forms\n* Show DataFrame via built-in visualisation\n\nThe ZeppelinContext is addressed via the pre-loaded object instance \"z.\" in IPython or PySpark interpreters.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n\n# dynamic form\nz.input(name\u003d\u0027my_name\u0027, defaultValue\u003d\u0027hello\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n\nimport pandas as pd\ndf \u003d pd.DataFrame({\u0027name\u0027:[\u0027a\u0027,\u0027b\u0027,\u0027c\u0027], \u0027count\u0027:[12,24,18]})\n\n# visualise the data frame via the context built-in\nz.show(df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualisation options\n\nOne big advantage of notebooks is that you can visualise data with your within your code and mark-down cells. [Matplotlib](https://matplotlib.org) is the premier Python plotting module available on this platform and it works in much the same way as other familiar Python environments (but note that an explicit call to `show()` is not necessary - plot rendering is accomplished via a post-execute hook which tells Zeppelin to plot all currently open matplotlib figures after executing the rest of the paragraph). Saving a plot locally is as simple as calling the pyplot instance savefig() method (see above for instructions on transfering files off the platform).\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nimport matplotlib.pyplot as plt\n\nplt.plot([1,2,3,4])\nplt.ylabel(\u0027some numbers\u0027)\n\n# to save the plot file use the savefig method, substituting your username as appropriate:\n# plt.savefig(\u0027/user/{YOUR-USERNAME}/somenums.png\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "To iteratively update a single plot, we can leverage Zeppelin\u0027s built-in Angular Display System. Currently this feature is only available for the `pyspark` interpreter for raster (png and jpg) formats. To enable this, we must set a special `angular` flag to `True` in our configuration:\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nimport matplotlib.pyplot as plt\nplt.close() # Added here to reset the plot when rerunning the paragraph\nz.configure_mpl(angular\u003dTrue, close\u003dFalse)\nplt.plot([1, 2, 3], label\u003dr\u0027$y\u003dx$\u0027)\n\n# ... the following related cells are placed by the side of this one by adjusting their width via the cog icon in the top right of each."
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nplt.plot([3, 2, 1], label\u003dr\u0027$y\u003d3-x$\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nplt.xlabel(r\u0027$x$\u0027, fontsize\u003d20)\nplt.ylabel(r\u0027$y$\u0027, fontsize\u003d20)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nplt.legend(loc\u003d\u0027upper center\u0027, fontsize\u003d20)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nplt.title(\u0027Inline plotting example\u0027, fontsize\u003d20)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Pandas provides a high level api for visualisation of Pandas data frames. It uses matplotlib for its visualization under the hood, so the usage is the same as matplotlib. "
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python.ipython\n\nimport pandas as pd\nimport numpy as np\n\nts \u003d pd.Series(np.random.randn(1000), index\u003dpd.date_range(\u00271/1/2000\u0027, periods\u003d1000))\nts \u003d ts.cumsum()\nts.plot()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Pandas User Defined Functions (a.k.a. vectorized UDFs)\n\nA convenient feature of the PySpark SQL data frame API is the programmability afforded by user defined functions. Users who have found themselves limited by the small number of aggregate functions typically available in ADQL will find this feature particularly useful in scale-out usage scenarios. There are illustrations of the use of UDFs in the tutorial notebooks provided on this platform - see for example notebook 5, \"Working with Gaia XP spectra\". \n\nFor further details see the [Apache Spark documentation for Pandas UDFs](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html#pandas-udfs-a-k-a-vectorized-udfs).\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ]
}