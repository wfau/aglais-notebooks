{
  "metadata": {
    "name": "2. Data holdings",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003c!--\n\n    Gaia Data Processing and Analysis Consortium (DPAC) \n    Co-ordination Unit 9 Work Package 930\n    \n    (c) 2005-2025 Gaia DPAC\n    \n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see \u003chttps://www.gnu.org/licenses/\u003e.\n    --\u003e\n\nThe science exploitation platform currently hosts the following data sets:\n\n* The Gaia EDR3 and DR3 source catalogues (direct copies of `gaiaedr3.gaia_source` and `gaiadr3.gaia_source` from the Gaia archive);\n* Best neighbour tables for PanSTARRS DR1, 2MASS PSC and ALLWISE (again, directly mirrored from the corresponding tables in the Gaia archive under the gaiaedr3 / gaiadr3 namespace) along with the original records from the 2MASS PSC and ALLWISE catalogues, and Object Thin / Object Mean columns from PanSTARRS DR1 (respectively 2MASS, ALLWISE and PS1 hereafter) where a best neighbour is available.\n* bulk data products (astrophysical parameters and spectra) from Gaia DR3.\n\nThese are presented as queryable resources as demonstrated in the following cells and subsequent notebooks. Click on the outward-pointing arrows in the top right of each cell to show the code; click on inward-pointing arrows to hide. Each cell can be run by pressing the play icon in the top-right; click on the cog icon to see other options, e.g. to clear current output. Note that the SQL-like API for querying here is [PySpark SQL](https://spark.apache.org/docs/3.0.0/sql-ref.html), not ADQL."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport gaiadmpsetup\n\n# show the database(s) available within the context\nspark.sql(\"show databases\").show()\n\n# show the contents of the catalogue store that has been set up\nprint(\"Tables available in current database context:\")\nfor line in spark.catalog.listTables(): print (line)\n\n# show the first 20 records from the Gaia DR3 source catalogue\nprint(\"\\nFirst 20 entries in gaia_source:\")\nspark.sql(\"select * from gaiadr3.gaia_source\").show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The database context is Gaia DR3 (`gaiadr3.`) at start-up so all that is needed to identify a DR3 table is the name (the first item in each record above) in queries. To change from the default context, e.g. to Gaia EDR3, simply\n\n    spark.sql(\"USE gaiaedr3\")\nor preface table names with the database name (`gaiaedr3.gaia_source`). Be aware that some tables (notably `gaia_source`) exist with the same name in different versions (i.e. different column and/or row sets) in different databases so be sure to use the correct one for your workflow - it is probably a good idea to always prefix table names with the required database name just to be sure.\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Column descriptions for `gaia_source` are browsable within the Gaia archive ADQL search tree-view, and also (for Gaia DR3 for example) [online](https://gea.esac.esa.int/archive/documentation/GDR3/Gaia_archive/chap_datamodel/sec_dm_main_source_catalogue/ssec_dm_gaia_source.html), as are the [best neighbour table columns](https://gea.esac.esa.int/archive/documentation/GDR3/Gaia_archive/chap_datamodel/sec_dm_cross-matches/). For the \"external\" catalogues the column sets can be browsed as follows (click on each to follow the links):\n\n* [2MASS PSC](https://old.ipac.caltech.edu/2mass/releases/allsky/doc/sec2_2a.html \"2MASS point-source catalogue column details\");\n* [ALLWISE](https://wise2.ipac.caltech.edu/docs/release/allwise/expsup/sec2_1a.html \"ALLWISE catalogue column details\");\n* [PS1 OTMO](https://outerspace.stsci.edu/display/PANSTARRS/PS1+MeanObjectView+table+fields \"PS1 DR1 object-thin object-mean catalogue column details\") (note that column names follow the Gaia archive convention of all lower-case with underscore separators rather than the original CamelCase).\n\nIt is important to note that the best neighbour tables should not be used as a proxy for the original external survey datasets: only those records that are the best neighbour to one or more Gaia sources are included here. If you wish to query and analyse one or other of those catalogues in isolation, this is not the facility to use! The phrase \"one or more\" is also noteworthy: because of the way the Gaia cross-matches have been derived any one external catalogue source can be the best neighbour to more than one Gaia source. Please consult the [Gaia cross-match documentation on-line](https://gea.esac.esa.int/archive/documentation/GDR3/Catalogue_consolidation/chap_crossmatch/) to familiarise yourself with the details.\n\nNote also the syntax in the last query above. At first sight this might look insanely profligate, especially if you are familiar with existing relational systems serving Gaia data via ADQL/TAP. On this Spark platform the execution engine works rather differently: a \"lazy\" execution plan is assembled from the workflow as notebook statements and cells are \"played\" through this interface - nothing actually happens until absolutely necessary. So the select statement is not executed: rather an optimised plan is created from the combination of the query as couched in SQL here, and the \".show()\" action. The latter is short-hand for displaying the first 20 entries only, and this is factored into the execution resulting in fast turn-around and a quick look at the underlying data resource. In general, all filters and other optimisations are \"pushed down\" through the execution plan at the last possible moment (when data are required to be collected from the distributed workers to the master executor in order to action an operation). This is discussed further in other example notebooks.\n\nColumn details can also be examined within the notebook by defining a data frame object against any table then echoing the schema; subsequent cells demonstrate some simple queries: \n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# define a data frame object on a table:\ndf \u003d spark.sql(\"select * from gaiaedr3.gaia_source_ps1_best_neighbours\")\n\n# display the schema with a touch of friendly formatting:\nprint(\"Column details of gaiaedr3.gaia_source_ps1_best_neighbours:\\n\")\nfor field in df.schema: print(\"%30s : %10s\"%(field.name, field.dataType.simpleString()))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# querying the main catalogue table is as simple as forming an SQL query:\ndf \u003d spark.sql(\"select source_id, ra, dec, phot_g_mean_mag AS G from gaiadr3.gaia_source\")\n\ndf.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# a simple SQL join on Gaia source_id can be used to pull in external catalogue data with Gaia data via the pre-computed best-neighbour joins:\n#query \u003d \"SELECT g.source_id, ra, dec, phot_g_mean_mag AS G, g_mean_psf_mag AS g_ps1, r_mean_psf_mag AS r_ps1\" + \\\n    #\" FROM gaia_source AS g INNER JOIN gaia_source_ps1_best_neighbours AS p ON g.source_id \u003d p.source_id\"\n\nquery\u003d \"SELECT gaia.source_id, gaia.ra, gaia.dec, gaia.phot_g_mean_mag AS gaia_g, ps1.g_mean_psf_mag AS ps1_g, ps1.r_mean_psf_mag AS ps1_r FROM gaiaedr3.gaia_source AS gaia INNER JOIN gaiaedr3.gaia_source_ps1_best_neighbours AS ps1 ON gaia.source_id \u003d ps1.source_id\"\n\n# define a data frame object on this query\ndf \u003d spark.sql(query)\n\n# echo first 20 records of the results set\ndf.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "* although SQL-based, the API is much richer than SQL/ADQL in terms of programmability and flexibility in column types - this will become increasingly important in future data releases.\n* the are no ADQL geometric functions to limit spatially selections - the platform is designed primarily for large-scale statistical analyses and does not replicate all features of existing relational systems, especially those needed to delimit row sets to small sub-samples.\n\n\n\n"
    }
  ]
}