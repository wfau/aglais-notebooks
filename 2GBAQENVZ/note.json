{
  "paragraphs": [
    {
      "text": "%python\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom collections import Counter\n\nclass MLlib_confusion_matrix():\n    \u0027\u0027\u0027subclass to plot confusion matrix\u0027\u0027\u0027\n    def __init__(self, df, labelCol \u003d \u0027label\u0027, classes \u003d None, normalized \u003d True):\n        self.df               \u003d df\n        self.getLabelCol      \u003d labelCol\n        self.grouped          \u003d df.groupBy(labelCol, \u0027prediction\u0027).count().toPandas()\n        self.matrix           \u003d self.getConfusionMatrix()\n        self.normMatrix       \u003d self.getNormConfusionMatrix()\n#         self.confusion_matrix \u003d self.confusionMatrix(classes, normalized)\n\n    def __repr__(self) : return f\"\"\"Class for plotting confusion matrix {self.normMatrix}\"\"\"\n\n    def getConfusionMatrix(self):\n        \u0027\u0027\u0027returns confusion matrix based on values\u0027\u0027\u0027\n\n        N_classes \u003d len(set(self.grouped[self.getLabelCol]))\n        matrix \u003d np.empty((N_classes, N_classes))\n        for i in range(N_classes):\n            for j in range(N_classes):\n                c \u003d self.grouped[(self.grouped[self.getLabelCol] \u003d\u003d i) \u0026 \n                                 (self.grouped.prediction \u003d\u003d j)][\u0027count\u0027].values\n                if len(c) \u003d\u003d 0:\n                    c \u003d 0 \n                matrix[i][j] \u003d  c\n        return matrix\n\n    def getNormConfusionMatrix(self):\n        \u0027\u0027\u0027returns confusion matrix based on values\u0027\u0027\u0027\n\n        N_classes \u003d len(set(self.grouped[self.getLabelCol]))\n        matrix \u003d np.empty((N_classes, N_classes))\n        for i in range(N_classes):\n            for j in range(N_classes):\n                c \u003d self.grouped[(self.grouped[self.getLabelCol] \u003d\u003d i) \u0026 \n                        (self.grouped.prediction \u003d\u003d j)][\u0027count\u0027].values\\\n                            /sum(self.grouped[(self.grouped[self.getLabelCol] \u003d\u003d i)][\u0027count\u0027])\n                if len(c) \u003d\u003d 0:\n                    c \u003d 0 \n                matrix[i][j] \u003d  np.round(c , 4)\n        return matrix\n    \n    def confusionMatrix(self, classes \u003d None, normalized \u003d True):\n        \u0027\u0027\u0027prints rich version of confusion matrix\u0027\u0027\u0027\n\n        if normalized: matrix \u003d self.normMatrix\n        else: matrix \u003d self.matrix\n        if classes \u003d\u003d None:\n            classes \u003d range(len(matrix))\n\n        N_classes \u003d range(len(classes))\n        plt.rcParams[\u0027figure.figsize\u0027] \u003d (6,6)\n\n        plt.imshow(matrix, cmap \u003d \u0027Greens\u0027, alpha \u003d 0.75)\n        for i in N_classes: # Add values to max pooling\n            for j in N_classes:\n                text \u003d plt.text(j, i, matrix[i][j],\n                               ha\u003d\"center\", va\u003d\"center\", color\u003d\"k\", fontsize \u003d 20)\n\n        #Add thick line to matrix\n        axis \u003d plt.gca()\n        axis.set_yticks(np.arange(-0.5, len(classes)-0.5, 1), minor\u003d\u0027True\u0027)\n        axis.set_xticks(np.arange(-0.5, len(classes)-0.5, 1), minor\u003d\u0027True\u0027)\n        axis.yaxis.grid(True, which\u003d\u0027minor\u0027, color \u003d \u0027k\u0027, lw \u003d 2)\n        axis.xaxis.grid(True, which\u003d\u0027minor\u0027, color \u003d \u0027k\u0027, lw \u003d 2)\n        plt.xticks(N_classes, classes, rotation \u003d0, fontsize \u003d 14)\n        plt.yticks(N_classes, classes, rotation \u003d0, fontsize \u003d 14)\n        plt.xlabel(\u0027Predicted Class\u0027, fontsize \u003d 16)\n        plt.ylabel(\u0027True Class\u0027, fontsize \u003d 16)\n        \n        \nclass MLlibMultiClassEvaluator(MLlib_confusion_matrix):\n    \u0027\u0027\u0027class to calculate parameters of NN performance for binary classification\u0027\u0027\u0027\n    \n    def __init__(self, df, labelCol \u003d \u0027label\u0027):\n        self.df            \u003d df\n        self.getLabelCol   \u003d labelCol\n        self.show          \u003d df.show\n        self.count         \u003d df.count\n        self.shape         \u003d (df.count(), len(df.columns))\n        self.grouped       \u003d df.groupBy(labelCol, \u0027prediction\u0027).count().toPandas()\n        self.matrix        \u003d self.getConfusionMatrix()\n        \n    def __len__(self)         : return self.df.count()\n    def __repr__(self)        : return f\"\"\"Evaluate ML performance for {len(self)} datapoints.\"\"\"\n    \n    def getAnalysis(self):\n        \u0027\u0027\u0027returns a pd.DataFrame for analysis\u0027\u0027\u0027\n        data \u003d self.df.select(self.getLabelCol,\u0027probability\u0027, \u0027prediction\u0027).toPandas()\n        for i in range(max(data[self.getLabelCol])+1):\n            data[f\u0027prob\u0027] \u003d [max(j) for j in data.probability]\n        return data\n    \n    def getTruePositives(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculates how True Positives change by threshold\u0027\u0027\u0027\n        conMat \u003d self.getConfusionMatrix()\n        data \u003d self.getAnalysis()\n        tp \u003d {}\n        for cat in range(len(conMat)):\n            tp[cat] \u003d []\n        for cat in range(len(conMat)):\n            for threshold in step:\n                tmp \u003d Counter(data[(data[\u0027prediction\u0027] \u003d\u003d data[self.getLabelCol]) \u0026 \n                                   (data[f\u0027prob\u0027]\u003ethreshold)][self.getLabelCol])\n                for cat in range(len(conMat)):\n                    tp[cat].append(tmp[cat])\n            return tp\n        \n        \n    def getFalsePositives(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculates how True Positives change by threshold\u0027\u0027\u0027\n        conMat \u003d self.getConfusionMatrix()\n        data \u003d self.getAnalysis()\n        fp \u003d {}\n        for cat in range(len(conMat)):\n            fp[cat] \u003d []\n        for cat in range(len(conMat)):\n            for threshold in step:\n                tmp \u003d Counter(data[(data[\u0027prediction\u0027] !\u003d data[self.getLabelCol]) \u0026 \n                                   (data[f\u0027prob\u0027]\u003ethreshold)].prediction)\n                for cat in range(len(conMat)):\n                    fp[cat].append(tmp[cat])\n            return fp        \n        \n    def getFalseNegatives(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculates how False Positives change by threshold\u0027\u0027\u0027\n        conMat \u003d self.getConfusionMatrix()\n        data \u003d self.getAnalysis()\n        fn \u003d {}\n        for cat in range(len(conMat)):\n            fn[cat] \u003d []\n        for cat in range(len(conMat)):\n            for threshold in step:\n                tmp \u003d Counter(data[(data[\u0027prediction\u0027] !\u003d data[self.getLabelCol]) \u0026 \n                                   (data[f\u0027prob\u0027]\u003ethreshold)][self.getLabelCol])\n                for cat in range(len(conMat)):\n                    fn[cat].append(tmp[cat])\n            return fn\n        \n    def getPrecision(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculate precision\u0027\u0027\u0027\n        precision \u003d {}; true_positives \u003d self.getTruePositives(step); \n        false_positives \u003d self.getFalsePositives(step)\n        conMat \u003d self.getConfusionMatrix()\n        precision \u003d {}\n        for cat in range(len(conMat)):\n            precision[cat] \u003d conMat[cat][cat]\n            for cat in range(len(conMat)):\n                precision[cat] \u003d [tp / (tp+fp) for tp, fp in zip(true_positives[cat],false_positives[cat])]\n        return precision\n    \n    def getRecall(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculate recall\u0027\u0027\u0027\n        conMat \u003d self.getConfusionMatrix()\n        recall \u003d {}; true_positives \u003d self.getTruePositives(step); \n        false_negatives \u003d self.getFalseNegatives(step)\n        for cat in range(len(conMat)):\n            recall[cat] \u003d []\n            for cat in range(len(conMat)):\n                recall[cat] \u003d [tp / (tp+fn) for tp, fn in zip(true_positives[cat],false_negatives[cat])]\n        return recall\n    \n    \nclass plottingThreshold(MLlibMultiClassEvaluator):\n    \u0027\u0027\u0027subclass of \"threshold\" for various plots of useful threshold parameters.\u0027\u0027\u0027\n    \n    def __repr__(self): return f\"\"\"Plotting software for \u0027threshold\u0027 objects\"\"\"\n    def __init__(self, df, labelCol \u003d \u0027label\u0027):\n        self.df            \u003d df\n        self.getLabelCol   \u003d labelCol\n        self.show          \u003d df.show\n        self.count         \u003d df.count\n        self.shape         \u003d (df.count(), len(df.columns))\n        self.grouped       \u003d df.groupBy(labelCol, \u0027prediction\u0027).count().toPandas()\n        self.matrix        \u003d self.getConfusionMatrix()\n        \n        \n    def getAxes(self,ax):\n        if ax \u003d\u003d None:\n            ax \u003d plt.subplot(111)\n        return ax\n\n    def plot_true_positives(self, ax \u003d None, step \u003d np.arange(0.0, 0.95, 0.01), normalized \u003d True, legend \u003d True):\n        ax \u003d self.getAxes(ax); true_positives \u003d self.getTruePositives(step); \n        conMat \u003d self.getConfusionMatrix()\n        for cat in range(len(conMat)):\n            if normalized \u003d\u003d True:\n                y \u003d [i/max(true_positives[cat]) for i in true_positives[cat]]\n                ax.set_ylabel(\u0027Fraction of Total True Positives --\u003e\u0027)\n            else:\n                y \u003d [i/len(self.data[self.data.true_label \u003d\u003d cat]) for i in true_positives[cat]]\n                ax.set_ylabel(\u0027True Positives --\u003e\u0027)\n            ax.plot(step, y, label \u003d cat)\n        if legend:\n            ax.legend()\n        ax.grid(\u0027on\u0027)\n        ax.set_xlabel(\u0027Threshold\u0027)\n\n    def plot_false_positives(self, ax \u003d None, step \u003d np.arange(0.0, 0.95, 0.01), legend \u003d True):\n        ax \u003d self.getAxes(ax); false_positives \u003d self.getFalsePositives(step)\n        conMat \u003d self.getConfusionMatrix()\n        for cat in range(len(conMat)):\n            if max(false_positives[cat]) !\u003d 0:\n                y \u003d [i/max(false_positives[cat]) for i in false_positives[cat]]\n            else: y \u003d [i for i in false_positives[cat]]\n            ax.plot(step, y, label \u003d cat)\n        if legend:\n            ax.legend()\n        ax.grid(\u0027on\u0027)\n        ax.set_xlabel(\u0027Threshold\u0027)\n        ax.set_ylabel(\u0027\u003c-- Fraction of Total False Positives\u0027)\n\n    def plot_precision(self, ax \u003d None, step \u003d np.arange(0.0, 0.95, 0.01), legend \u003d True):\n        ax \u003d self.getAxes(ax); precision \u003d self.getPrecision(step)\n        conMat \u003d self.getConfusionMatrix()\n        for cat in range(len(conMat)):\n            y \u003d precision[cat]\n            ax.plot(step, y, label \u003d cat)\n        if legend:\n            ax.legend()\n        ax.grid(\u0027on\u0027)\n        ax.set_xlabel(\u0027Threshold\u0027)\n        ax.set_ylabel(\u0027Precision --\u003e\u0027)\n\n    def plot_recall(self, ax \u003d None, step \u003d np.arange(0.0, 0.95, 0.01), legend \u003d True):\n        ax \u003d self.getAxes(ax); recall \u003d self.getRecall(step)\n        conMat \u003d self.getConfusionMatrix()\n        for cat in range(len(conMat)):\n            y \u003d recall[cat]\n            ax.plot(step, y, label \u003d cat)\n        if legend:\n            ax.legend()\n        ax.grid(\u0027on\u0027)\n        ax.set_xlabel(\u0027Threshold\u0027)\n        ax.set_ylabel(\u0027Recall --\u003e\u0027)\n        \n\n    def threshold_subplots(self, step \u003d np.arange(0.0, 0.95, 0.01), figsize\u003d(15, 8)):\n        import matplotlib.gridspec as gridspec\n        fig \u003d plt.figure(figsize \u003d figsize)\n        gs \u003d gridspec.GridSpec(ncols\u003d2, nrows\u003d2, figure\u003dfig)\n        ax1 \u003d fig.add_subplot(gs[0, 0])\n        ax2 \u003d fig.add_subplot(gs[0, 1])\n        ax3 \u003d fig.add_subplot(gs[1, 0])\n        ax4 \u003d fig.add_subplot(gs[1, 1])\n\n        axs \u003d [ax1, ax2, ax3, ax4]\n        self.plot_precision      (ax \u003d axs[0], step \u003d step, legend \u003d False)\n        self.plot_recall         (ax \u003d axs[1], step \u003d step, legend \u003d False)\n        self.plot_true_positives (ax \u003d axs[2], step \u003d step, legend \u003d True )\n        self.plot_false_positives(ax \u003d axs[3], step \u003d step, legend \u003d True )\n        plt.tight_layout()\n\n\n",
      "user": "dcr",
      "dateUpdated": "2021-07-09 14:13:43.699",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625839984336_-1847158797",
      "id": "20210709-141304_540532340",
      "dateCreated": "2021-07-09 14:13:04.336",
      "dateStarted": "2021-07-09 14:13:17.835",
      "dateFinished": "2021-07-09 14:13:21.252",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%python\n",
      "user": "dcr",
      "dateUpdated": "2021-07-09 14:13:17.830",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1625839997829_-839258675",
      "id": "20210709-141317_881285977",
      "dateCreated": "2021-07-09 14:13:17.829",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "experiments/dcr/mllib_results",
  "id": "2GBAQENVZ",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "python:shared_process": [],
    "sh:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}