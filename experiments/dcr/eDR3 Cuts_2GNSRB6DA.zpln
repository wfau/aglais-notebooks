{
  "paragraphs": [
    {
      "text": "%md\n\n# Using Spark to select required eDR3 data and perform a coordinate shift into LSR\n\nCurrent deployment has Spark 2.4.7 installed\n",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eUsing Spark to select required eDR3 data and perform a coordinate shift into LSR\u003c/h1\u003e\n\u003cp\u003eCurrent deployment has Spark 2.4.7 installed\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_446868311",
      "id": "20210222-101122_1782048334",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n\nimport numpy as np\nimport pandas as pd\nimport pyspark.ml as ml\nimport matplotlib.pylab as plt\nfrom collections import Counter\nimport pyspark.sql.functions as f\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.functions import lit, col, when, floor\n",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_532611385",
      "id": "20210709-154631_1954980489",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n\ncuts \u003d  \u0027(parallax_error \u003c 1 or parallax_over_error \u003c 0.03) AND\u0027 + \\\n         \u0027(phot_g_mean_flux_over_error*0.03 \u003e 1.0857) AND\u0027 + \\\n         \u0027(astrometric_excess_noise \u003c 1 or (astrometric_excess_noise \u003e 1 AND astrometric_excess_noise_sig \u003c 2)) AND\u0027 + \\\n         \u0027(b \u003e 30)\u0027\n\n\n# define the data source\ngs_df \u003d sqlContext.read.parquet(\u0027file:////user/nch/PARQUET/TESTS/GEDR3/*.parquet\u0027)\n# register as SQL-queryable \ngs_df.createOrReplaceTempView(\u0027gaia_source\u0027)\n\n",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-42-d8faec97b257\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# define the data source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m \u001b[0mgs_df\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027file:////user/nch/PARQUET/TESTS/GEDR3/*.parquet\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# register as SQL-queryable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027gaia_source\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027name\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027string\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u0027year\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027int\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u0027month\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027int\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u0027day\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027int\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib64/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_1537241017",
      "id": "20210222-095742_336765643",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\ngs_df.columns",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[\u0027solution_id\u0027,\n \u0027designation\u0027,\n \u0027source_id\u0027,\n \u0027random_index\u0027,\n \u0027ref_epoch\u0027,\n \u0027ra\u0027,\n \u0027ra_error\u0027,\n \u0027dec\u0027,\n \u0027dec_error\u0027,\n \u0027parallax\u0027,\n \u0027parallax_error\u0027,\n \u0027parallax_over_error\u0027,\n \u0027pm\u0027,\n \u0027pmra\u0027,\n \u0027pmra_error\u0027,\n \u0027pmdec\u0027,\n \u0027pmdec_error\u0027,\n \u0027ra_dec_corr\u0027,\n \u0027ra_parallax_corr\u0027,\n \u0027ra_pmra_corr\u0027,\n \u0027ra_pmdec_corr\u0027,\n \u0027dec_parallax_corr\u0027,\n \u0027dec_pmra_corr\u0027,\n \u0027dec_pmdec_corr\u0027,\n \u0027parallax_pmra_corr\u0027,\n \u0027parallax_pmdec_corr\u0027,\n \u0027pmra_pmdec_corr\u0027,\n \u0027astrometric_n_obs_al\u0027,\n \u0027astrometric_n_obs_ac\u0027,\n \u0027astrometric_n_good_obs_al\u0027,\n \u0027astrometric_n_bad_obs_al\u0027,\n \u0027astrometric_gof_al\u0027,\n \u0027astrometric_chi2_al\u0027,\n \u0027astrometric_excess_noise\u0027,\n \u0027astrometric_excess_noise_sig\u0027,\n \u0027astrometric_params_solved\u0027,\n \u0027astrometric_primary_flag\u0027,\n \u0027nu_eff_used_in_astrometry\u0027,\n \u0027pseudocolour\u0027,\n \u0027pseudocolour_error\u0027,\n \u0027ra_pseudocolour_corr\u0027,\n \u0027dec_pseudocolour_corr\u0027,\n \u0027parallax_pseudocolour_corr\u0027,\n \u0027pmra_pseudocolour_corr\u0027,\n \u0027pmdec_pseudocolour_corr\u0027,\n \u0027astrometric_matched_transits\u0027,\n \u0027visibility_periods_used\u0027,\n \u0027astrometric_sigma_5d_max\u0027,\n \u0027matched_transits\u0027,\n \u0027new_matched_transits\u0027,\n \u0027matched_transits_removed\u0027,\n \u0027ipd_gof_harmonic_amplitude\u0027,\n \u0027ipd_gof_harmonic_phase\u0027,\n \u0027ipd_frac_multi_peak\u0027,\n \u0027ipd_frac_odd_win\u0027,\n \u0027ruwe\u0027,\n \u0027scan_direction_strength_k1\u0027,\n \u0027scan_direction_strength_k2\u0027,\n \u0027scan_direction_strength_k3\u0027,\n \u0027scan_direction_strength_k4\u0027,\n \u0027scan_direction_mean_k1\u0027,\n \u0027scan_direction_mean_k2\u0027,\n \u0027scan_direction_mean_k3\u0027,\n \u0027scan_direction_mean_k4\u0027,\n \u0027duplicated_source\u0027,\n \u0027phot_g_n_obs\u0027,\n \u0027phot_g_mean_flux\u0027,\n \u0027phot_g_mean_flux_error\u0027,\n \u0027phot_g_mean_flux_over_error\u0027,\n \u0027phot_g_mean_mag\u0027,\n \u0027phot_bp_n_obs\u0027,\n \u0027phot_bp_mean_flux\u0027,\n \u0027phot_bp_mean_flux_error\u0027,\n \u0027phot_bp_mean_flux_over_error\u0027,\n \u0027phot_bp_mean_mag\u0027,\n \u0027phot_rp_n_obs\u0027,\n \u0027phot_rp_mean_flux\u0027,\n \u0027phot_rp_mean_flux_error\u0027,\n \u0027phot_rp_mean_flux_over_error\u0027,\n \u0027phot_rp_mean_mag\u0027,\n \u0027phot_bp_n_contaminated_transits\u0027,\n \u0027phot_bp_n_blended_transits\u0027,\n \u0027phot_rp_n_contaminated_transits\u0027,\n \u0027phot_rp_n_blended_transits\u0027,\n \u0027phot_proc_mode\u0027,\n \u0027phot_bp_rp_excess_factor\u0027,\n \u0027bp_rp\u0027,\n \u0027bp_g\u0027,\n \u0027g_rp\u0027,\n \u0027dr2_radial_velocity\u0027,\n \u0027dr2_radial_velocity_error\u0027,\n \u0027dr2_rv_nb_transits\u0027,\n \u0027dr2_rv_template_teff\u0027,\n \u0027dr2_rv_template_logg\u0027,\n \u0027dr2_rv_template_fe_h\u0027,\n \u0027l\u0027,\n \u0027b\u0027,\n \u0027ecl_lon\u0027,\n \u0027ecl_lat\u0027]"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_67220630",
      "id": "20210222-133919_237410028",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n# clear any previously cached data in the context (cells may be executed in any order, and out-dated by changes from here onwards)\nsqlContext.clearCache()\n\n# a conservative selection of everything that COULD be within 100pc, including things with measured \n# distances putting them outside the 100pc horizon when their true distances are within, and also including \n# loads of spurious chaff with the wheat of course, plus bad things with significant, unphysical parallaxes:\nraw_sources_df \u003d spark.sql(\u0027SELECT source_id, random_index, parallax, ra, dec, b, pmra, pmdec, phot_g_mean_flux_over_error, \u0027 + cuts + \u0027FROM gaia_source WHERE (parallax \u003e 1)\u0027)\n\n# cache it for speedy access below (all subsequent samples are derived from this):\nraw_sources_df.cache()\n\n# register as SQL-queryable\nraw_sources_df.createOrReplaceTempView(\u0027raw_sources\u0027)\n\nraw_sources_df.count()\n# parallax \u003e 10: count \u003d 574531\n# parallax \u003e 1:  count \u003d 217,066,354\n# with b \u003e 30",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o67.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: gaia_source; line 1 pos 315\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:798)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:780)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:719)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view \u0027gaia_source\u0027 not found in database \u0027default\u0027;\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:795)\n\t... 63 more\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-38-5b623b602485\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# distances putting them outside the 100pc horizon when their true distances are within, and also including\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# loads of spurious chaff with the wheat of course, plus bad things with significant, unphysical parallaxes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 7\u001b[0;31m \u001b[0mraw_sources_df\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027SELECT source_id, random_index, parallax, ra, dec, b, pmra, pmdec, phot_g_mean_flux_over_error, \u0027\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcuts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\u0027FROM gaia_source WHERE (parallax \u003e 1)\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# cache it for speedy access below (all subsequent samples are derived from this):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34mu\u0027row1\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34mu\u0027row2\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34mu\u0027row3\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[0;32m-\u003e 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027org.apache.spark.sql.AnalysisException: \u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027: \u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027org.apache.spark.sql.catalyst.analysis\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027: \u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: \u0027Table or view not found: gaia_source; line 1 pos 315\u0027"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_750833516",
      "id": "20210222-101319_841382328",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\nraw_sources_df.groupby().min(\u0027phot_g_mean_flux_over_error\u0027).collect()[0]",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Row(min(phot_g_mean_flux_over_error)\u003d6.709118843078613)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_869301612",
      "id": "20210222-122107_1366721724",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n\nfrom pyspark.sql.functions import lit, col, cos, sin, max\nfrom numpy import pi\nimport numpy as np\nimport pandas as pd",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_2015711103",
      "id": "20210222-102909_9166953",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n# Matrix multiplication of sources\n\ndef matrix_multiplication_array_dataframe(A, B, B_name \u003d \u0027B\u0027, name \u003d \u0027C\u0027, drop_B \u003d True):\n    \u0027\u0027\u0027return A with A*B appended as columns, e.g. C01\n    \n    INPUTS\n         A - matrix A as an array\n         B - matric B as dataframe\n    B_name - columns prefix of B\n      name - desired prefix of output, default \u003d \u0027C\u0027\n    drop_A - if True, only return new columns, e.g. C--\n    \u0027\u0027\u0027\n    \n    n,m \u003d np.shape(A)[0], 3\n    M \u003d np.zeros((n,m))\n    \n    for i in range(n):\n        for j in range(3):\n            element \u003d [ A[i][k]*col(\u0027{}{}{}\u0027.format(B_name,k,j)) for k in range(3)]\n            if len(element) \u003c 3:\n                element.append(0)\n\n            B \u003d B.withColumn(\u0027{}{}{}\u0027.format(name,i,j), element[0]+element[1]+element[2])\n    if drop_B \u003d\u003d True:\n        res \u003d B.select([i for i in B.columns if name in i])\n        return res\n    else:\n        return B\n\n    \ndef matrix_multiplication_dataframes(df, A_name \u003d \u0027A\u0027, B_name \u003d \u0027B\u0027, name \u003d \u0027C\u0027, drop_A \u003d True):\n    \u0027\u0027\u0027multipies two matricies in the same dataframe and adds new matrix to end\u0027\u0027\u0027\n    \n    n,m \u003d 3, 1\n    M \u003d np.zeros((n,m))\n    \n    for i in range(n):\n        for j in range(m):\n#             print([ \u0027{}{}{}\u0027.format(A_name,i,k)+\u0027{}{}{}\u0027.format(B_name,k,j) for k in range(3)])\n            element \u003d [ col(\u0027{}{}{}\u0027.format(A_name,i,k))*col(\u0027{}{}{}\u0027.format(B_name,k,j)) for k in range(3)]\n            \n            \n            df \u003d df.withColumn(\u0027{}{}{}\u0027.format(name,i,j), element[0]+element[1]+element[2])\n    if drop_A \u003d\u003d True:\n        res \u003d df.select([i for i in df.columns if name in i])\n        return res\n    return df\n\ndef reshape_input(Input, padding \u003d 1, desired_length \u003d 2):\n    \u0027\u0027\u0027reshapes input into list of lists padded/cut to a standard desired length\n    \n    EXAMPLE\n        input \u003d [[cos, cos], [sin], [cos, sin, cos],\n                 [sin, cos], [cos], [sin, sin],\n                   [sin],     [0],    [cos]]\n                   \n        new_input \u003d  reshape_input(input, padding \u003d 1) \n        \n        new_input \u003d [[cos, cos], [sin, 1], [cos, sin],\n                     [sin, cos], [cos, 1], [sin, sin],\n                      [sin, 1],   [0, 1],   [cos, 1]]\n        \u0027\u0027\u0027\n    \n    for i in Input:\n        if len(i) \u003c desired_length:\n            while len(i) \u003c desired_length:\n                i.append(padding)\n        elif len(i)\u003edesired_length: \n            while len(i) \u003e desired_length:\n                i.pop()\n    return Input\n\ndef element_transform(function, col_name, sign \u003d 1, padding \u003d 1, col_units \u003d \u0027degrees\u0027):\n    \u0027\u0027\u0027Returns the spark syntax to select and transform columns from a dataframe\n    \u0027\u0027\u0027\n    \n    if function \u003d\u003d padding:\n        return 1\n    \n    elif type(function) \u003d\u003d int: \n        return lit(function)\n    \n    else:\n        if col_units \u003d\u003d \u0027degrees\u0027:\n            conv\u003dpi/180\n        else:\n            conv \u003d 1\n        return sign*function(col(col_name)*conv)\n    \ndef drop_matrix_from_dataframe(df, name):\n    \u0027\u0027\u0027Drops matrix from the dataframe, assuming the column names\n    use function: \"create_matrix_index\" notation. \u0027\u0027\u0027\n    drop \u003d [i for i in df.columns if name in i]\n    df \u003d df.select([column for column in df.columns if column not in drop])\n    return df\n\ndef create_matrix_index(N,M, matrix_name \u003d \u0027A\u0027):\n    \u0027\u0027\u0027Creates index for matrix column names.\u0027\u0027\u0027\n    indicies\u003d[]\n    for i in range(M):\n        for j in range(N):\n            indicies.append(\u0027{}{}{}\u0027.format(matrix_name,i,j))\n    return indicies\n\ndef drop_DataFrame(df, matrix_cols, signs, names, drop_df \u003d True):\n    \u0027\u0027\u0027Drops input DataFrame columns after calulation\u0027\u0027\u0027\n    \n    if drop_df \u003d\u003d True:\n        A \u003d df.select([c for c in matrix_cols])\n        for i in range(len(signs)):\n            A \u003d A.withColumnRenamed(A.columns[i], names[i])\n        return A\n    else:\n        for i in range(len(matrix_cols)):\n            df \u003d df.withColumn(names[i], matrix_cols[i])\n            \n        index \u003d len(df.columns) - len(signs)\n        for i in range(len(signs)):\n            df \u003d df.withColumnRenamed(df.columns[index+i], names[i])\n        return df",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_16921233",
      "id": "20210222-105826_1240882787",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n\ndef create_Matrix_A(df, drop_df \u003d True, coord_units \u003d \u0027degrees\u0027):\n    \u0027\u0027\u0027creates Matrix A, where A \u003d [[cos(ra)*cos(dec), -sin(ra), -cos(ra)*sin(dec)],\n                                     [sin(ra)*cos(dec),  cos(ra), -sin(ra)*sin(dec)],\n                                     [sin(dec),             0,     cos(dec)]])\u0027\u0027\u0027\n\n    # creating the filter for df\n    trig_funcs \u003d [[cos, cos], [sin], [cos, sin],\n                       [sin, cos], [cos], [sin, sin],\n                       [sin], [0], [cos]]\n    trig_funcs \u003d  reshape_input(trig_funcs)\n\n    cols \u003d [[\u0027ra\u0027, \u0027dec\u0027], [\u0027ra\u0027], [\u0027ra\u0027, \u0027dec\u0027],\n            [\u0027ra\u0027, \u0027dec\u0027], [\u0027ra\u0027], [\u0027ra\u0027, \u0027dec\u0027],\n            [\u0027dec\u0027], [], [\u0027dec\u0027]]\n    cols \u003d reshape_input(cols)\n\n    signs \u003d [1, -1, -1,\n             1,  1, -1, \n             1,  1,  1]\n\n    matrix_cols\u003d[]\n    for i in range(len(signs)):\n        a \u003d element_transform(trig_funcs[i][0], cols[i][0], sign \u003d signs[i], col_units\u003dcoord_units) * \\\n            element_transform(trig_funcs[i][1], cols[i][1], col_units\u003dcoord_units)\n        matrix_cols.append(a)\n\n    names \u003d create_matrix_index(3,3, \u0027A\u0027)\n    \n    df \u003d drop_DataFrame(df \u003d df, matrix_cols \u003d matrix_cols, signs \u003d signs,\n                        names \u003d names, drop_df \u003d drop_df)\n    return df\n\ndef form_matrix_C(df, radial_velocity\u003d0, drop_df \u003d True):\n    \u0027\u0027\u0027Selects the correct columns for C depending on given units of columns.\u0027\u0027\u0027\n    \n    if radial_velocity \u003d\u003d 0:\n        df \u003d df.withColumn(\u0027C00\u0027, lit(radial_velocity))\\\n            .withColumnRenamed(\u0027pmra (km/s)\u0027,  \u0027C10\u0027)\\\n            .withColumnRenamed(\u0027pmdec (km/s)\u0027, \u0027C20\u0027)\n    else:\n        df \u003d df.select(col(\u0027radial_velocity\u0027), col(\u0027pmra (km/s)\u0027), col(\u0027pmdec (km/s)\u0027))\\\n            .withColumnRenamed(\u0027radial_velocity\u0027, \u0027C00\u0027)\\\n            .withColumnRenamed(\u0027pmra (km/s)\u0027,     \u0027C10\u0027)\\\n            .withColumnRenamed(\u0027pmdec (km/s)\u0027,    \u0027C20\u0027)\n    if drop_df \u003d\u003d True:      \n        C \u003d df.select(\u0027C00\u0027, \u0027C10\u0027, \u0027C20\u0027)\n        return C\n    return df\n\ndef create_matrix_C(df, pm_units \u003d \u0027mas/yr\u0027, drop_df \u003d True):\n    \u0027\u0027\u0027 Calls \"form_Matrix_C\" with the correct input columns.\u0027\u0027\u0027\n    \n    if \u0027pmra (km/s)\u0027 in df.columns and \u0027pmra (km/s)\u0027 in df.columns:\n        C \u003d form_matrix_C(df, drop_df\u003ddrop_df)\n\n    elif pm_units \u003d\u003d \u0027mas/yr\u0027:\n        df \u003d convMasKm_sql(df, \u0027pmra\u0027, \u0027pmdec\u0027)\n        C  \u003d  from_matrix_C(df, drop_df\u003ddrop_df)\n\n    elif pm_units \u003d\u003d \u0027km/s\u0027:\n        df \u003d df.withColumnRenamed(\u0027pmra\u0027, \u0027pmra (km/s)\u0027)\\\n               .withColumnRenamed(\u0027pmdec\u0027, \u0027pmdec (km/s)\u0027)\n        C  \u003d form_matrix_C(df, drop_df\u003ddrop_df)\n        df \u003d df.withColumnRenamed(\u0027pmra (km/s)\u0027, \u0027pmra\u0027)\\\n               .withColumnRenamed(\u0027pmdec (km/s)\u0027, \u0027pmdec\u0027)\n    return C\n\ndef calculate_LSR_mag(df, name \u003d \u0027U\u0027):\n    \u0027\u0027\u0027 Calculates magnitude of LSR velocity give U,V,W velocity.\n    Uses v_sun from Schonrich, R., Binney, J., \u0026 Dehnen, W. 2010 | \n    DOI: 10.1111/j.1365-2966.2010.16253.x\u0027\u0027\u0027\n    \n    v_sun \u003d [11.1, 12.24, 7.25]\n    df \u003d df.withColumn(\u0027v_lsr^2\u0027, (col(\u0027{}00\u0027.format(name))+v_sun[0])**2 + \n                                  (col(\u0027{}10\u0027.format(name))+v_sun[1])**2 + \n                                  (col(\u0027{}20\u0027.format(name))+v_sun[2])**2)\n    df \u003d drop_matrix_from_dataframe(df, \u0027U\u0027)\n    return df",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.946",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038946_1118052331",
      "id": "20210222-110205_789379435",
      "dateCreated": "2021-12-02 09:57:18.946",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n\ndef convMasKm_sql(df, *args, add_to_data \u003d True):\n    \u0027\u0027\u0027\n    FUNCTION\n        Converts data from mas/yr to km/s\n    \n    REQUIRES:\n        df \u003d sql.DataFrame as an input with column [\u0027parallax\u0027 in mas/yr] and any args provided\n        *args \u003d columns names of df you wish to convert from mas/yr to km/s\n    \n    RETURNS\n    \n        if add_to_data \u003d False - returns first column in *args [km/s] as an sql.DataFrame\n        if add_to_data \u003d True  - returns df with all *args [km/s] added.\n    \u0027\u0027\u0027\n    \n    for i in args:\n        df \u003d df.withColumn(str(i + \u0027 (km/s)\u0027), col(i)/col(\u0027parallax\u0027)*4.74057)\n        \n        if add_to_data \u003d\u003d False:   # Returns first column in args as np.array\n            return df.select(str(args[0] + \u0027 (km/s)\u0027))\n            break\n    \n    return df\n\ndef LSR_conv_sql(df, cut \u003d 60, coord_units \u003d \u0027degrees\u0027):\n    \u0027\u0027\u0027\n    FUNCTION\n        Converts proper motions from ra, dec to LSR velocity\n    \n    REQUIRES:\n        df \u003d dataframe of ra, dec, parallax, pmra, pmdec values respectively\n        \n    OPTIONAL:\n        coord_units \u003d [defaut \u003d \u0027deg\u0027] units of coordinates (ra, dec). \n                      Accepted values are [\u0027deg\u0027, \u0027rad\u0027]\n        pm_units \u003d [defaut \u003d \u0027mas/yr\u0027] units of proper motions. \n                    Accepted values are: [\u0027mas/yr\u0027, \u0027km/s\u0027]\n        radial_velocity \u003d list of radial velocity (defaults \u003d 0.0)\n        mag \u003d Return only V_LSR mag (default \u003d False - returns three dimensions of v_lsr [u,v,w])\n    \n    SEE ALSO\n         Method from: Johnson, Dean R. H., Soderblom, David R. DOI: 10.1086/114370\n    \u0027\u0027\u0027\n    \n    T \u003d [[-0.05646624, -0.87325802, -0.48397519],\n         [ 0.49253617, -0.44602111,  0.74731071],\n         [-0.86845823, -0.19617746,  0.4552963 ]]\n\n    df \u003d create_Matrix_A(df, drop_df\u003dFalse, coord_units\u003dcoord_units)   # forms Marix A\n    df \u003d matrix_multiplication_array_dataframe(T,df, B_name \u003d \u0027A\u0027,\n                                               name \u003d \u0027B\u0027, drop_B \u003d False)  # forms Matrix B\n    \n    df \u003d drop_matrix_from_dataframe(df, \u0027A\u0027)   # drops A from df\n    df \u003d create_matrix_C(df, drop_df\u003dFalse)    # forms Matrix C\n    \n    df \u003d matrix_multiplication_dataframes(df, A_name \u003d \u0027B\u0027,\n                                          B_name \u003d \u0027C\u0027, name \u003d \u0027U\u0027, drop_A \u003d False) # Creates U, W, V\n    df \u003d drop_matrix_from_dataframe(df, \u0027B\u0027)   # drops B from df\n\n    df \u003d df.withColumnRenamed(\u0027C10\u0027, \u0027pmra (km/s)\u0027)\\\n        .withColumnRenamed(\u0027C20\u0027, \u0027pmdec (km/s)\u0027)   # Cleaning up DataFrame Columns\n    df \u003d drop_matrix_from_dataframe(df, \u0027C\u0027)   # drops C from df\n    df \u003d calculate_LSR_mag(df, name \u003d \u0027U\u0027)                 # Adds v_lsr^2 column to df\n    \n    return df\n\ndef LSR_cut_sql(df, cut, column, calc_lsr \u003d True, drop_lsr \u003d True):\n    \u0027\u0027\u0027   \n    FUNCTION\n        Cuts the data according to below a certain value\n    \n    INPUT:\n        df \u003d input pd.DataFrame\n        cut \u003d v_max^2 for the LSR velocity\n        calc_lsr \u003d calculate lsr velocity if not present - [True/False]\n        drop_lsr \u003d drop v_lsr from df - [True/False]\n        \n    RETURNS\n        DataFrame contiaining only objects which satisfy the cut value\u0027\u0027\u0027\n\n    if calc_lsr \u003d\u003d True:  df \u003d LSR_conv_sql(df)                   # Calcs the lsr velocity\n    if drop_lsr \u003d\u003d True:  df \u003d df.filter(col(column) \u003c cut).drop(col(column))\n    else: df \u003d df.filter(col(column) \u003c cut)\n    return df",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.947",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038947_963295158",
      "id": "20210222-110221_53619910",
      "dateCreated": "2021-12-02 09:57:18.947",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n# Convert to Cartesian Coordinates\n\ndef deg_conv_factor(unit \u003d \u0027degrees\u0027):\n    \u0027\u0027\u0027return conversion factor to radians\u0027\u0027\u0027\n    \n    if unit \u003d\u003d \u0027degrees\u0027: return pi/180\n    elif unit \u003d\u003d \u0027radians\u0027: return 1\n    else: raise ValueError(\u0027Please enter a valid unit for angle [\"degrees\", \"radians\"]\u0027)\n        \ndef plx_conv_factor(unit \u003d \u0027mas\u0027):\n    \u0027\u0027\u0027return conversion factor to arcsec\u0027\u0027\u0027\n    \n    if unit \u003d\u003d \u0027mas\u0027: return 10**-3\n    elif unit \u003d\u003d \u0027arcsec\u0027: return 1\n    else: raise ValueError(\u0027Please enter a valid unit for parallax [\"mas\", \"arcsec\"]\u0027)\n        \ndef equatorial_to_cartesian(df, drop_df \u003d True, coord_units \u003d \u0027degrees\u0027):\n    \u0027\u0027\u0027Converts equatorial coordinates to cartesian from a spark dataframe\n    \n    RETURNS\n        sql.DataFrame with cartesian coordinates in parsec\u0027\u0027\u0027\n    \n    trig_funcs \u003d [[1, cos, cos], [1, cos, sin], [1, sin]]\n    trig_funcs \u003d reshape_input(trig_funcs, desired_length \u003d 3)\n\n    cols \u003d [[\u0027parallax\u0027,\u0027dec\u0027, \u0027ra\u0027], [\u0027parallax\u0027,\u0027dec\u0027,\u0027ra\u0027], [\u0027parallax\u0027,\u0027dec\u0027]]\n    cols \u003d reshape_input(cols, desired_length \u003d 3)\n    signs \u003d [1, 1, 1]\n\n    matrix_cols\u003d[]\n    plx_conv \u003d plx_conv_factor(\u0027mas\u0027)\n    \n    for i in range(len(signs)):\n        a \u003d 1/(col(cols[i][0])*plx_conv) * \\\n            element_transform(trig_funcs[i][1], cols[i][1], col_units \u003d \u0027degrees\u0027) * \\\n            element_transform(trig_funcs[i][2], cols[i][2], col_units \u003d \u0027degrees\u0027)\n\n        matrix_cols.append(a)\n\n    names \u003d [\u0027X_pc\u0027, \u0027Y_pc\u0027, \u0027Z_pc\u0027]\n    df \u003d drop_DataFrame(df \u003d df, matrix_cols \u003d matrix_cols, signs \u003d signs,\n                        names \u003d names, drop_df \u003d drop_df)\n\n    return df",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.947",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038947_1463338985",
      "id": "20210222-110245_1593163512",
      "dateCreated": "2021-12-02 09:57:18.947",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\ndef quality_cut(df, cut_lsr \u003d 60, cartesian \u003d True, drop_equitorial \u003d False):\n    \u0027\u0027\u0027\n    FUNCTION\n        perform the relevant control cuts on the data.\n    \n    INPUT\n        df - full data release sql.DataFrame\n        cut_lsr - value of maximum LSR velocity (m/s)\n    RETURNS\n        sql.DataFrame containing only objects of \u0027\u0027\u0027\n    \n    \n    df \u003d convMasKm_sql(df, \u0027pmra\u0027, \u0027pmdec\u0027)      # converts proper motions to km/s.\n    df \u003d LSR_conv_sql(df)\n    # calculates v_lsr**2 and drops objects above the cut\n    df \u003d LSR_cut_sql(df, cut_lsr**2, \u0027v_lsr^2\u0027, calc_lsr \u003d True, drop_lsr \u003d True) \n    \n    if cartesian \u003d\u003d True:\n        df \u003d equatorial_to_cartesian(df, drop_df \u003d False)\n        \n    if drop_equitorial \u003d\u003d True:\n        df \u003d df.drop(\u0027ra\u0027).drop(\u0027dec\u0027)\n    return df\n\ndf \u003d quality_cut(df \u003d raw_sources_df, cut_lsr \u003d 60, drop_equitorial \u003d False)\ndf.show(1), df.count()",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.947",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-------------------+-----------------+-----------------+\n|         source_id|random_index|          parallax|                ra|               dec|                 b|               pmra|             pmdec|       pmra (km/s)|     pmdec (km/s)|               X_pc|             Y_pc|             Z_pc|\n+------------------+------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-------------------+-----------------+-----------------+\n|782897221514589696|  1197047509|2.7839847671176337|163.17428838259454|45.303381748390976|60.221768121789516|-0.7145026520745194|-5.146548191477398|-1.216655305500019|-8.76354362575473|-241.82675577445272|73.13023066035508|255.3322120314242|\n+------------------+------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-------------------+-----------------+-----------------+\nonly showing top 1 row\n\n(None, 201523600)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038947_584557722",
      "id": "20210222-110319_292516891",
      "dateCreated": "2021-12-02 09:57:18.947",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\ndf.show(20)",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.947",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+\n|         source_id|random_index|          parallax|                ra|               dec|                 b|               pmra|              pmdec|        pmra (km/s)|       pmdec (km/s)|               X_pc|              Y_pc|              Z_pc|\n+------------------+------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+\n|782897221514589696|  1197047509|2.7839847671176337|163.17428838259454|45.303381748390976|60.221768121789516|-0.7145026520745194| -5.146548191477398| -1.216655305500019|  -8.76354362575473|-241.82675577445272| 73.13023066035508| 255.3322120314242|\n|782897599471715968|   446169514|2.7226583322838254| 163.1001841997849| 45.31078760308174|60.172089575139154|  8.812108140245474| -6.794019706716884| 15.343245603410763|-11.829440961861152|-247.14456264202522| 75.08739140863689|261.11682374178235|\n|782897706855057280|   369921591| 1.185784475098683| 163.0389892571423| 45.29136192125652| 60.14275456761751|-3.3637244005852103| 2.1825498752268455|-13.447613218544777|   8.72547303433288| -567.4741035699025|173.07207624270976|  599.344514019152|\n|782897878653178368|   854314206|4.4942995681485804|163.08498072152997| 45.32457751399142| 60.15625694753797| -48.11369782320196| -46.74867811836684| -50.75014449552955|-49.310326930182896|-149.67231215613415| 45.51678489101521|158.22290544969937|\n|782897913012937088|   486514911|1.8918589080089998| 163.1242901362674| 45.31069478126761| 60.18718642086109|  5.249729868822031|-14.040037154235769| 13.154634216582531| -35.18115365289113|-355.72275236110596|107.91212999131723|375.78423715126064|\n|782898290970037760|   280207438|1.2271503289116836|163.13090503951133|45.358062940806114| 60.16949095482562| -5.146438456292712|  7.222127135209507|-19.881061983974227| 27.899596672661712| -547.9672051729253| 166.1623647702011| 579.8082800505867|\n|782898668927179776|  1044408055| 5.633288492253828|163.25111882070402|  45.3412931999292|60.252156529863875|-52.573310705283674|  5.067555152731487| -44.24191302697387|  4.264489554088659|-119.47974341178079| 35.95677682411784|126.26838273713895|\n|782898703286917376|    40649202|2.6935258951618986|163.26959145654354| 45.34820701093627| 60.26045534429367|  8.146285956598913|-3.6792163327581258| 14.337355689299178| -6.475372151391489|-249.87579706729457| 75.11090782387055| 264.1114531108524|\n|782899218682969984|   758013833|2.4877222543389155|  163.319346101251| 45.38123071568943| 60.27606091954607| 12.444322144561387|-10.971995914734016|  23.71373256236764| -20.90808754184363|-270.46004688672093| 81.04237133916213|286.12357797289417|\n|782900451330045312|   884833457| 1.438343725809936| 163.2687133015707| 45.44041081902645| 60.21704008671083|-14.614167139607881| -10.18479833991497| -48.16615185497432| -33.56760181858693|-467.16692124500605|140.43489918904515|495.37609537237364|\n|782900794927392384|  1780467333|1.3471544319781008|163.07054687347687| 45.33253351656679|60.143588098578405| -5.675877595546235|   5.77745657465338|-19.973133305592693| 20.330584722858113| -499.2198124347024|151.95493442525665| 527.9266740617687|\n|782900971029632128|  1520912757|2.5629281838376015| 163.0442235273544|45.349917719161674|60.119166939615454|-21.627559069517176| -5.210368927678215|-40.003835591156616|  -9.63746030155975| -262.2881246478067|  79.9682102989922|277.57782136449237|\n|782901172884531712|   900653565| 6.055731076810283|163.07926436123262| 45.38034251773985|60.127025391841826|    6.8039795007222| -49.19024424355338|  5.326316623479931|-38.507290564245004| -110.9674996803158| 33.75837579832385|117.53908734117141|\n|782901314627013888|   494338733|3.1913905810039154|163.12686221329884|45.377473435848174|60.158012750287654|-25.960130891506196| 17.519788786383916| -38.56181641722924| 26.024324826120715|-210.62700540102938| 63.88552795301887| 223.0218810582351|\n|782901516481920384|   372744185|1.8334979893725143|163.12091599484793| 45.39469549426828|60.146354365379686|  -8.58943716980627| 3.3058884813741893|-22.208275329499482|  8.547484561742806| -366.4946072491466|111.20345164150676| 388.3075089098554|\n|782901589504921216|   540980297|1.3141082155850217|163.10402513602858| 45.39096471557551| 60.13755634213988|  5.079848739628322|-4.7890039622062694|  18.32526290759027|-17.276057058215184|-511.33638562007036| 155.3166348434265| 541.7478578589585|\n|782902448498363904|   949546313|1.6791279611561347| 163.1184905153011|45.418874302686056|  60.1336669903519|-12.430202403618585|-0.6633526313632365| -35.09336153746705|-1.8727992484244083| -400.0117614729308|121.39187098526268| 424.1828652696253|\n|782902585937318144|    20933628| 4.303584964742039|163.07833371648994| 45.42677134286013| 60.10502365226298| -10.06884121443219| -2.805756752339918|-11.091229053673839| -3.090652652709347|-156.01736420961694| 47.46615057489507| 165.5257311647523|\n|782903170052868352|   912512781| 4.424009183957052|163.18956406894395|45.439379164107834|60.168360715230534| -8.370899902910075| 10.137439079009702| -8.969881232760942| 10.862825454579246|-151.82590840819674|45.869120504631525|161.05492443346532|\n|782903268828591744|   983728773|1.6854082787949478|163.20647246940337| 45.44208311372329|60.177609665021215|  1.105301910837148|-11.214790169523821| 3.1088972003885265|-31.543987592105392| -398.5425283219713|120.27795953761931|  422.770901548933|\n+------------------+------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038947_1052985693",
      "id": "20210222-110329_1521870143",
      "dateCreated": "2021-12-02 09:57:18.947",
      "status": "READY"
    },
    {
      "text": "%spark.pyspark\n",
      "user": "admin",
      "dateUpdated": "2021-12-02 09:57:18.947",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439038947_1186221060",
      "id": "20210222-115741_973868268",
      "dateCreated": "2021-12-02 09:57:18.947",
      "status": "READY"
    }
  ],
  "name": "eDR3 Cuts",
  "id": "2GNSRB6DA",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}