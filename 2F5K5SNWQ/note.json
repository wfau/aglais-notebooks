{
  "paragraphs": [
    {
      "text": "%md\nSee [this link](https://arxiv.org/abs/1907.07709).\n\nUpdate: Current method updated to neaten code and made use of tidy functions, input is currently pd.DataFrame, working to include to sql.DataFrame functionality.",
      "user": "admin",
      "dateUpdated": "2020-09-01 13:59:43.202",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eSee \u003ca href\u003d\"https://arxiv.org/abs/1907.07709\"\u003ethis link\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eUpdate: Current method updated to neaten code and made use of tidy functions, input is currently pd.DataFrame, working to include to sql.DataFrame functionality.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1583849639182_820137816",
      "id": "20200310-141359_226957878",
      "dateCreated": "2020-03-10 14:13:59.182",
      "dateStarted": "2020-09-01 13:59:43.203",
      "dateFinished": "2020-09-01 13:59:44.450",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh pip install pandas\n# -- ONLY run if error importing pandas! Must restart notebook if install required.\n",
      "user": "gaiauser",
      "dateUpdated": "2020-05-29 14:22:05.646",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in /home/fedora/.local/lib/python3.7/site-packages (1.0.3)\nRequirement already satisfied: numpy\u003e\u003d1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas) (1.17.4)\nRequirement already satisfied: pytz\u003e\u003d2017.2 in /usr/lib/python3.7/site-packages (from pandas) (2018.5)\nRequirement already satisfied: python-dateutil\u003e\u003d2.6.1 in /usr/lib/python3.7/site-packages (from pandas) (2.8.0)\nRequirement already satisfied: six\u003e\u003d1.5 in /usr/lib/python3.7/site-packages (from python-dateutil\u003e\u003d2.6.1-\u003epandas) (1.12.0)\nWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\nYou should consider upgrading via the \u0027/usr/bin/python3 -m pip install --upgrade pip\u0027 command.\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586427387438_739466625",
      "id": "20200409-101627_1921515986",
      "dateCreated": "2020-04-09 10:16:27.438",
      "dateStarted": "2020-05-29 14:22:05.659",
      "dateFinished": "2020-05-29 14:22:06.278",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n",
      "user": "gaiauser",
      "dateUpdated": "2020-05-29 14:22:15.631",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1587475001920_1721514592",
      "id": "20200421-131641_782582910",
      "dateCreated": "2020-04-21 13:16:41.920",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# define the data frame source on the given column selection/predicates:\ndf \u003d sqlContext.read.parquet(\n    \"/hadoop/gaia/parquet/gdr2/gaia_source/*.parquet\"\n    ).select(\n    [\"designation\",\"source_id\",\"ra\",\"ra_error\",\"dec\",\"dec_error\",\"parallax\",\"parallax_error\",\"parallax_over_error\",\"pmra\",\"pmra_error\",\"pmdec\",\"pmdec_error\",\"l\",\"b\"]\n    ).where(\n    \"abs(b) \u003c 30.0 AND parallax \u003e 1.0 and parallax_over_error \u003e 10.0 AND phot_g_mean_flux_over_error \u003e 36.19 AND astrometric_sigma5d_max \u003c 0.3 AND visibility_periods_used \u003e 8 AND (astrometric_excess_noise \u003c 1 OR (astrometric_excess_noise \u003e 1 AND astrometric_excess_noise_sig \u003c 2))\"\n    )\n\n# sanity check\ndf.show()\nprint (\"Data frame rows: \",df.count())\n\n\n\n\n",
      "user": "gaiauser",
      "dateUpdated": "2020-05-29 14:22:19.442",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+-------------------+------------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+\n|         designation|          source_id|                ra|            ra_error|                dec|           dec_error|          parallax|      parallax_error|parallax_over_error|                pmra|          pmra_error|              pmdec|         pmdec_error|                 l|                  b|\n+--------------------+-------------------+------------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+\n|Gaia DR2 40396662...|4039666296672658688|272.42180060744215|0.040014912399424604|  -33.6058067001021| 0.03368056050988915| 1.422454370334301| 0.03639988616495275|           39.07854| -0.2316953688797376| 0.07537877095615261| -5.442599071096166| 0.05727019630225524|358.50569414878646|-6.7818037118031835|\n|Gaia DR2 40396673...|4039667327461865216|272.65050482164776|0.036197874417770184| -33.67244054531187| 0.03680133647540661|1.1673137239965692| 0.05314058363370944|           21.96652| -1.7009801509860536| 0.07029452441830816| -4.612361250689048| 0.06085092511704771|358.53690441856696| -6.981138715828702|\n|Gaia DR2 40396638...|4039663822768546432| 272.3987836204827|0.027632276638390182|-33.677978944324124|0.024215370325550427| 1.485547221111856|0.028698903012696158|          51.763206|  -4.537954548081503|0.053742838174843835|-15.585752841174445|0.041699306844425514| 358.4325162212831| -6.799004597798456|\n|Gaia DR2 40396743...|4039674302495016832| 272.6370332645537|0.049971393914832546| -33.46345401655811| 0.04311907194434805|1.4895944391179747| 0.05535383094438027|          26.910412|  -8.999004938426811| 0.08901429663519801|-20.520372339141726| 0.07278953875661565| 358.7173313727555| -6.872875125255238|\n|Gaia DR2 40396798...|4039679804475284224| 272.3352958925287|  0.0382698356897684| -33.49684273645387|   0.037562716485539|1.8288406168863958|0.049118014784077936|            37.2336|  -8.709127050643408| 0.07206191453025368| 0.8317008127357967| 0.05431303815619029|  358.568061282215|  -6.66676412739173|\n|Gaia DR2 40396768...|4039676845112356736| 272.3423038848421| 0.04004613278050688| -33.60277422977108| 0.03534959354180454| 1.331757953116398|0.040852709757309896|           32.59901| -3.0736631636887344| 0.07887016066616566|-10.535097449648152| 0.06167703651701124|358.47687324908384| -6.722021120385618|\n|Gaia DR2 40396799...|4039679976215199616| 272.3870770186676| 0.08175560039135911| -33.49054936039203|   0.075806277430834|1.6828015366409843| 0.09616438158987135|          17.499218|  0.9078068372385885|    0.15609879018991|-1.2743414940340876| 0.12356941230805032| 358.5942080511929|  -6.70183491892182|\n|Gaia DR2 40396779...|4039677948992976896| 272.2946282491588| 0.12673632257759757|-33.562874682533035| 0.11352057025821762|1.2791078599019605| 0.12717607561652478|          10.057771|  2.4788330269377497|  0.2613657929145823| -4.650451848991806| 0.20943502287503454| 358.4933431825897| -6.668151666197993|\n|Gaia DR2 40396646...|4039664686186893184| 272.4026652887885|0.028595834471134035|-33.655006326453886| 0.02471972176840793|1.4230023257454893|0.029119862707883994|           48.86707|  -4.179903049739455|0.055444952571015844| 2.2287270254996123|  0.0428403985184529| 358.4544442439153| -6.791000148338951|\n|Gaia DR2 40396753...|4039675333287149952|  272.560428244972| 0.14485939823543023| -33.41907979107032| 0.13639184748166955|2.4892891798856938| 0.14768574894726139|           16.85531|   2.753665468378671|  0.2764468222685262| -6.187195733058044|  0.2210087660210865|358.72641382590615| -6.795584005240619|\n|Gaia DR2 40396696...|4039669629564951040| 272.7381011527819|0.054428306834246126| -33.56432279582611| 0.04383693249181095|1.4039064198829385| 0.06633427846464744|          21.164116|   3.411131784353724| 0.09135111973361876| -9.646276318124418| 0.08216672365634985| 358.6675940515556| -6.994684164951169|\n|Gaia DR2 40396626...|4039662693254567936| 272.5126080410794|  0.1296423098745303| -33.65215003086476| 0.10917065938921705|1.2421645377888288|  0.1122515594329213|            11.0659|    7.70196276687754| 0.25578905728633683|-17.583724821676032| 0.20088518616997741| 358.5004815041314| -6.870335004176007|\n|Gaia DR2 40396725...|4039672520147052672|272.57023824221136|  0.0736686882516885| -33.51129861096017| 0.06447426562797175| 1.186762299958647| 0.08862786004612552|          13.390398|   2.632117122829801| 0.13686967280780654| 0.3072041918179895|  0.1083246074218071|358.64839146395377| -6.846275843225122|\n|Gaia DR2 40396727...|4039672721944070272| 272.5831112426574| 0.07043976875751465|-33.483537099952784| 0.06291530701296215|  1.14022685446635| 0.08033686117429235|          14.193072|  0.5481922043982299|   0.129302666462865|-3.2025426136016226| 0.10287713081563385|358.67814785261555|-6.8426601585483455|\n|Gaia DR2 40396731...|4039673134263578112|272.47253543660037|0.041702049821903266| -33.51891353312088| 0.03645176703075055|1.1267320071311797|0.047071092260350056|          23.936815|  -1.641507160720163| 0.07769872412607842| -4.671616157042623| 0.06032524035452944| 358.6029327525877| -6.778040244101522|\n|Gaia DR2 40396646...|4039664686186892160|272.40377537872985| 0.04377600533015369| -33.65486867142623|0.037860082819238965|  1.75887165621122|  0.0445294022348985|          39.499107| -0.9566730190113414| 0.08496727540942774| -3.746117897740997|   0.066416625862686|358.45500600935554|  -6.79174955438742|\n|Gaia DR2 40396815...|4039681591181083520| 272.1513758092714| 0.03374330430351806| -33.54770223115408| 0.03215174942793003|1.2770089043898252| 0.03768576974690001|          33.885704| -14.528880635531543| 0.06242652192511705|-1.5617009216815694| 0.05084214045259791|  358.449834180826| -6.555840447573484|\n|Gaia DR2 40396683...|4039668396976095232| 272.5918785069274| 0.06755164731288285| -33.62795490984999|0.060064042374027175| 1.278165908009941| 0.08163770487003097|          15.656564|  -9.059932437739432| 0.12441941818286029| 1.0153309974276419| 0.10066735403770635|358.55330353126215| -6.917141240695328|\n|Gaia DR2 40396782...|4039678253864546816| 272.2796609111014| 0.12881518153071386| -33.51657315885373| 0.11029532166270983|1.1790261056103815| 0.11170275683028853|          10.555032|  -9.837527608593797|  0.2374980846729896| -4.101407578324116|  0.1907102657065912|358.52845560549713| -6.635240808816544|\n|Gaia DR2 40396815...|4039681591181086848| 272.1520450526198| 0.04506019510179999| -33.54255938796362| 0.04309540098315705|1.0121130032598475| 0.04860285626079909|          20.824146|-0.01420160186318746| 0.08474653494512939|-4.1072215733651944| 0.07023090482212226|358.45465735183967| -6.553891056963974|\n+--------------------+-------------------+------------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+\nonly showing top 20 rows\n\n(\u0027Data frame rows: \u0027, 21901470)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1583849193678_2137442757",
      "id": "20200310-140633_496300561",
      "dateCreated": "2020-03-10 14:06:33.678",
      "dateStarted": "2020-05-29 14:22:19.457",
      "dateFinished": "2020-05-29 14:25:32.215",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# test: can we write to the file store?\n\n# Create some sample text, and turn into an RDD\nsometext\u003d\"TEST\"\nrdd \u003d sc.parallelize([sometext])\n\n# Then save as text file , using below if underline storage is HDFS\nrdd.saveAsTextFile(\"hdfs:///hadoop/temp/testN.txt\")\n\n# Read Text File we just wrote into a new rdd and print\nmy_rdd \u003d sc.textFile(\"hdfs:///hadoop/temp/testN.txt\")\n\nprint(my_rdd)\nmy_rdd.collect()\t\n\n# YES!\n",
      "user": "gaiauser",
      "dateUpdated": "2020-05-29 14:32:23.918",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o105.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://stv-dev-master:9000/hadoop/temp/testN.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling o105.saveAsTextFile.\\n\u0027, JavaObject id\u003do106), \u003ctraceback object at 0x7f04742fdf00\u003e)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586275166220_-1822178194",
      "id": "20200407-155926_429939943",
      "dateCreated": "2020-04-07 15:59:26.221",
      "dateStarted": "2020-05-29 14:32:23.945",
      "dateFinished": "2020-05-29 14:32:24.028",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nimport pandas as pd\n\nprint(type(df))\n#df \u003d pd.DataFrame(df, columns\u003ddf.columns)\n#print(type(df))\n#df.head()\n# ... doesn\u0027t work for me for the system install of pandas. Try this:\npandas_df \u003d df.select(\"*\").toPandas()\nprint(type(pandas_df))\npandas_df.head()\n\n\n# df.select(\u0027designation\u0027).show()",
      "user": "gaiauser",
      "dateUpdated": "2020-05-29 14:33:35.511",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cclass \u0027pyspark.sql.dataframe.DataFrame\u0027\u003e\n"
          },
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o156.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3901 in stage 5.0 failed 4 times, most recent failure: Lost task 3901.3 in stage 5.0 (TID 17187, stv-dev-worker-8, executor 1): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling o156.collectToPython.\\n\u0027, JavaObject id\u003do163), \u003ctraceback object at 0x7f0468b30cd0\u003e)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586424897715_590911086",
      "id": "20200409-093457_1719054744",
      "dateCreated": "2020-04-09 09:34:57.715",
      "dateStarted": "2020-05-29 14:33:35.531",
      "dateFinished": "2020-05-29 14:43:54.641",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n#import csv\nimport time\nimport numpy as np\n#import pandas as pd\nfrom numpy import cos,sin,pi\n\ndef insert_to_df(df, name, insert_data, index\u003d-1):\n    \u0027\u0027\u0027Creates column in DataFrame if does not exist, or replaces values if already present.\u0027\u0027\u0027\n    if name in df.columns:\n        df[name] \u003d insert_data\n    else:\n        if index\u003d\u003d-1:               # Adds the data to end of the DataFrame\n            index\u003dlen(df.columns)\n        df.insert(index, name, insert_data)\n    return df\n\ndef convMasKm(df, *args, add_to_data\u003dFalse, index\u003d-1):\n    \u0027\u0027\u0027\n    NAME\n        convMasKm\n        \n    FUNCTION\n        Converts data from mas/yr to km/s\n    \n    REQUIRES:\n        df \u003d pd.DataFrame as an input with column [\u0027parallax\u0027 in mas/yr] and any args provided\n        *args \u003d columns names of df you wish to convert from mas/yr to km/s\n        \n    OPTIONAL\n        add_to_data \u003d [boolean] Add results to df or return as arrays.\n        index \u003d column index to add in df, defaults to end on the DataFrame\n    \n    RETURNS\n    \n        if add_to_data \u003d False - returns first column in *args [km/s] as an array.\n        if add_to_data \u003d True  - returns df(DataFrame) with all *args [km/s] added.\n    \u0027\u0027\u0027\n    \n    c\u003d0;\n    for i in args:\n        kms \u003d np.array(df[i].values, dtype\u003dfloat)/\\\n                   (np.array(df[\u0027parallax\u0027].values,dtype\u003dfloat))*4.74057;\n        \n        if add_to_data \u003d\u003d False:   # Returns first column in args as np.array\n            return kms\n            break\n        \n        else:\n            if index \u003d\u003d -1:\n                print(-1)\n                df\u003dinsert_to_df(df, str(i + \u0027 (km/s)\u0027), kms, index\u003dindex)\n\n            else:\n                if type(index)\u003d\u003dint:\n                    index\u003d[index]\n                df\u003dinsert_to_df(df, str(i + \u0027 (km/s)\u0027), kms, index\u003dindex[c])\n#                 data.insert(index[c], str(i + \u0027 (km/s)\u0027), kms)\n                c+\u003d1\n    return df\n\n\ndef LSR_conv(ra, dec, parallax, pmra, pmdec,\n             coord_u \u003d \u0027deg\u0027, pm_units\u003d\u0027mas/yr\u0027, radial_velocity\u003d0.0, mag\u003dFalse):\n    \u0027\u0027\u0027\n    NAME\n        LSR_conv\n        \n    FUNCTION\n        Converts data from ra, dec and parallax to LSR coordinates\n    \n    REQUIRES:\n        ra, dec, parallax, pmra, pmdec \u003d list of ra, dec, parallax, pmra, pmdec values respectively\n        \n    OPTIONAL:\n        coord_units \u003d [defaut \u003d \u0027deg\u0027] units of coordinates (ra, dec). \n                      Accepted values are [\u0027deg\u0027, \u0027rad\u0027]\n        pm_units \u003d [defaut \u003d \u0027mas/yr\u0027] units of proper motions. \n                    Accepted values are: [\u0027mas/yr\u0027, \u0027km/s\u0027]\n        radial_velocity \u003d list of radial velocity (defaults \u003d 0.0)\n        mag \u003d Return only V_LSR mag (default \u003d False - returns three dimensions of v_lsr [u,v,w])\n        \n    RETURNS\n        three lists containing u,v,w for all objects in input\n    \n    SEE ALSO\n         Method from: Johnson, Dean R. H., Soderblom, David R. DOI: 10.1086/114370\n    \u0027\u0027\u0027\n       \n    k\u003d4.74057\n    v_sun \u003d np.array((11.1, 12.24, 7.25))\n    T\u003d np.array([[-0.05646624, -0.87325802, -0.48397519],\n                 [ 0.49253617, -0.44602111,  0.74731071],\n                 [-0.86845823, -0.19617746,  0.4552963 ]])\n    \n    if coord_u \u003d\u003d \u0027deg\u0027:\n        a\u003dra*pi/180     # deg to rad\n        d\u003ddec*pi/180    # deg to rad\n    else:\n        # ra and dec already in rad\n        a\u003dra\n        d\u003ddec\n    \n    # define a coordinate matrix\n    A \u003d np.array([[cos(a)*cos(d), -sin(a), -cos(a)*sin(d)],\n              [sin(a)*cos(d), cos(a), -sin(a)*sin(d)],\n              [sin(d), 0, cos(d)]])\n    \n    B \u003d np.matmul(T,A)\n    if pm_units \u003d\u003d \u0027mas/yr\u0027:\n        C \u003d np.array([radial_velocity, k*pmra/(parallax), k*pmdec/(parallax)])\n    elif pm_units \u003d\u003d \u0027km/s\u0027:\n        C \u003d np.array([radial_velocity, pmra, pmdec])\n    else: \n        raise ValueError        \n        \n    U,V,W \u003d np.matmul(B,C)\n    u, v, w \u003d (np.array((U,V,W)).T + list(v_sun)).T\n    \n    if mag \u003d\u003d True:    # Returns velocity magnitude\n        res\u003d[np.linalg.norm((j,k,l)) for j,k,l in zip(u,v,w)]\n        return res\n    \n    else:           # Returns the velocity components\n        return [u,v,w]\n\ndef LSR_cut(df, cut):\n    \u0027\u0027\u0027\n    NAME\n        LSR_cut\n        \n    FUNCTION\n        Cuts the data according to below a certain value\n    \n    REQUIRES:\n        df \u003d input pd.DataFrame [requires columns - \u0027ra\u0027, \u0027dec\u0027, \u0027parallax\u0027, \u0027pmra\u0027, \u0027pmdec\u0027]\n        cut \u003d max value for the LSR velocity\n        \n    RETURNS\n        DataFrame contiaining all object which satisfy the cut value\u0027\u0027\u0027\n\n    ra,dec,parallax,pmra,pmdec\u003ddata.iloc[:][[\u0027ra\u0027,\u0027dec\u0027,\u0027parallax\u0027, \\\n                                                    \u0027pmra\u0027, \u0027pmdec\u0027]].values.T\n    res\u003dLSR_conv(ra,dec,parallax,pmra, pmdec, mag\u003dTrue)\n#     res\u003d[np.linalg.norm(j) for j in zip(*x)]   # for if res \u003d False\n    res\u003dpd.DataFrame({\u0027lsr\u0027:res})\n    data.drop(res[res.lsr\u003ecut].index, inplace\u003dTrue)\n        \n\n    return df\n    \ndata \u003d convMasKm(data, \u0027pmra\u0027, \u0027pmdec\u0027, add_to_data\u003dTrue, index\u003d[7,9])    # converts proper motions from mas/yr to km/s \nprint(data.columns)\ndata \u003d LSR_cut(data, 60)                                # Cuts data with velocity above 60 km/s in LSR frame.\n# print(len(data), \u0027LSR cut complete\u0027)\n\n# write the new selection with appended columns to the file store to avoid needing to recompute:\ndf.rdd.saveAsTextFile(\"hdfs:///hadoop/temp/kc_hdbscan_inputs.txt\")",
      "user": "gaiauser",
      "dateUpdated": "2020-07-09 15:29:31.674",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 10: pmra_kms \u003d np.array(df[\u0027pmra\u0027].values, dtype\u003dfloat)/(np.array(df[\u0027parallax\u0027].values,dtype\u003dfloat))*4.74057;\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9208543893914005383.py\", line 375, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 10, in \u003cmodule\u003e\nValueError: setting an array element with a sequence.\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1583850071664_1289701082",
      "id": "20200310-142111_1877616625",
      "dateCreated": "2020-03-10 14:21:11.664",
      "dateStarted": "2020-04-07 16:25:20.059",
      "dateFinished": "2020-04-07 16:25:20.287",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nimport hdbscan\n\ndef clustering_info(clusterer, add_to_data\u003dFalse, df\u003d[], index\u003d-1):\n    \u0027\u0027\u0027\n    REQUIRED\n        clusterer \u003d output clusterer from HDBSCAN.\n        \n    OPTIONAL\n        add_to_data \u003d [boolean] Add results to df or return as arrays.\n        df \u003d [Required if add_to_data \u003d True] DataFrame to add results.\n        index \u003d [Default \u003d -1] Column location to add results.\n        \n    RETURNS\n        *groups \u003d Cluster labels for each point in the dataset given to fit(). \n                   Noisy samples are given the label -1.\n        *prob \u003d The strength with which each sample is a member of its assigned cluster.\n        *persistence \u003d A score of how persistent each cluster is. \n                       A score of 1.0 represents a perfectly stable cluster. \n    \n        if add_to_data \u003d False, returns [groups, prob, persistence] as seperate arrays.\n        if add_to_data \u003d True - returns df(DataFrame), persistence (array) - Must provide df to add.\n        \n        (*From HDBSCAN - in SEE ALSO)\n        \n    SEE ALSO\n        https://hdbscan.readthedocs.io/en/latest/\n    \u0027\u0027\u0027\n    # probabilities and group for each object\n    prob\u003dclusterer.probabilities_\n    groups\u003dclusterer.labels_\n\n    # Info on persistence of groups\n    persistence\u003dclusterer.cluster_persistence_\n    \n    print(\u0027Number of Groups \u003d \u0027, max(groups)+1) \n    if add_to_data \u003d\u003d False:\n        return groups, prob, persistence\n    \n    if add_to_data \u003d\u003d True:\n        df\u003dinsert_to_df(df, \u0027group\u0027, groups, index\u003dindex)\n        df\u003dinsert_to_df(df,\u0027probability\u0027, prob, index\u003dindex)\n        return df, persistence\n    \ndef clustering_prediction(clusterer, points_to_predict, cols, add_to_data\u003dFalse, index\u003d-1):\n    \u0027\u0027\u0027\n    REQUIRED\n        clusterer \u003d output clusterer from HDBSCAN.\n        points_to_predict \u003d DataFrame of objects to predict relationships to clustered data.\n        cols \u003d list of columns used for clustering.\n    \n    OPTIONAL\n        add_to_data \u003d [boolean] Add results to points_to_predict or return as arrays.\n        index \u003d [Default \u003d -1] Column location to add results.\n        \n    RETURNS\n        *group \u003d The predicted labels of the points_to_predict\n        *prob \u003d The soft cluster scores for each of the points_to_predict\n        \n        if add_to_data\u003dFalse, returns [group, prob] as seperate arrays.\n        if add_to_data\u003dTrue - returns df(DataFrame) with group and prob added.\n        \n        (*From HDBSCAN - in SEE ALSO)\n        \n    SEE ALSO\n        https://hdbscan.readthedocs.io/en/latest/\n    \u0027\u0027\u0027\n    \n    groups, prob \u003d hdbscan.approximate_predict(clusterer, points_to_predict[cols])\n    if add_to_data \u003d\u003d False:\n        return groups, prob\n    \n    if add_to_data \u003d\u003d True:\n        points_to_predict\u003dinsert_to_df(points_to_predict, \u0027group\u0027, groups, index\u003dindex)\n        points_to_predict\u003dinsert_to_df(points_to_predict,\u0027probability\u0027, prob, index\u003dindex)\n        return points_to_predict\n        \n     \n        \n# Apply HDBSCAN for full data with prediction ON.\ncols\u003d[\u0027l\u0027, \u0027b\u0027, \u0027parallax\u0027,\u0027pmra (km/s)\u0027,\u0027pmdec (km/s)\u0027]\nclusterer \u003d hdbscan.HDBSCAN(min_cluster_size\u003d40, \n                            min_samples\u003d25,\n                            prediction_data\u003dTrue, \n                            allow_single_cluster\u003dFalse,\n#                             memory\u003d\u0027data/leaf_cache/\u0027,\n                            cluster_selection_method\u003d\u0027leaf\u0027,\n                            gen_min_span_tree\u003dTrue).fit(data[cols])\n\n# collect results from HDBSCAN\ndata, persistence \u003d clustering_info(clusterer, True, data)\n# print(data.columns)\n#Â data.to_csv(\"data/DR2_with_groups_leaf.csv\", index\u003dNone)\n\n# Collect prediction data from HDBSCAN\nwdcols\u003d[\u0027l\u0027,\u0027b\u0027,\u0027Plx\u0027, \u0027pmRA\u0027, \u0027pmDE\u0027]    # Naming WD columns\n# print(WDs.columns)\nWDs\u003dclustering_prediction(clusterer, WDs, wdcols,  True) \n# WDs.to_csv(\"data/WDs_with_groups_leaf.csv\", index\u003dNone)\nprint(\u0027WDs check complete\u0027)",
      "user": "admin",
      "dateUpdated": "2020-08-17 21:34:47.662",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 1: import hdbscan\nTraceback (most recent call last):\n  File \"/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1588261403747_0074/container_1588261403747_0074_01_000001/tmp/zeppelin_pyspark-8397244157700295955.py\", line 375, in \u003cmodule\u003e\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\nModuleNotFoundError: No module named \u0027hdbscan\u0027\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1584614418201_416940842",
      "id": "20200319-104018_1699124593",
      "dateCreated": "2020-03-19 10:40:18.201",
      "dateStarted": "2020-08-17 21:34:47.680",
      "dateFinished": "2020-08-17 21:34:47.691",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n",
      "user": "admin",
      "dateUpdated": "2020-08-17 21:34:47.664",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1597700087663_247087687",
      "id": "20200817-213447_295448304",
      "dateCreated": "2020-08-17 21:34:47.663",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Kounkel and Covey groups demo",
  "id": "2F5K5SNWQ",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "python:shared_process": [],
    "sh:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}